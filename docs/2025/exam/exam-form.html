<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>DT2112 exam submission, spring 2025</title>
<link rel="stylesheet" type="text/css" href="exam-form.css">
<script id="exam-form-script" type="text/javascript" src="exam-form.es"></script>
</head>
<body>
<h1>DT2112 exam submission, spring 2025</h1>
<form id="examForm">
<fieldset class="form-actions">
<legend>Form actions</legend>
<button type="button" onclick="saveFormData()">Save form data</button><label for="loadFile" class="custom-file-upload">Load form data</label><input type="file" id="loadFile" onchange="loadFormData(event)"><label class="custom-clear-save" onclick="clearAutoSavedForm()">
                Clear autosave
            </label>
</fieldset>
<fieldset>
<legend>Student Information</legend>
<label for="kth-user">KTH User (Email Name):</label><input type="text" id="kth-user" name="kth-user" placeholder="e.g., user123">
</fieldset>
<div class="question">
<h2>E level questions | 1 perception and speech recognition</h2>
<h2>Question</h2>
<p>Describe speech recognition in at least four distinct steps.</p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Preprocessing the speech signal to enhance features for recognition</li>
<li class="segment">
<span class="segment-number">2. </span>This step involves techniques like noise reduction, band-pass filtering, or
						spectral subtraction.</li>
<li class="segment">
<span class="segment-number">3. </span>These adjustments ensure that all speech sounds are
						perceived equally by the recognition system.</li>
<li class="segment">
<span class="segment-number">4. </span>To handle speech rate variability,
						the signal may be normalized through time-stretching or compression.</li>
<li class="segment">
<span class="segment-number">5. </span>Extracting relevant acoustic features from the preprocessed speech signal</li>
<li class="segment">
<span class="segment-number">6. </span>Once cleaned, the signal is analyzed to extract acoustic features that help
						identify speech patterns.</li>
<li class="segment">
<span class="segment-number">7. </span>High-frequency components carry the most linguistic
						information, and are prioritized over low-frequency features.</li>
<li class="segment">
<span class="segment-number">8. </span>These features are
						typically spectral, derived from techniques such as Mel-Frequency Cepstral Coefficients
						(MFCCs) or spectrogram analysis.</li>
<li class="segment">
<span class="segment-number">9. </span>Using statistical models to map acoustic features to phonetic units</li>
<li class="segment">
<span class="segment-number">10. </span>Once features are extracted, statistical models like Hidden Markov Models (HMMs)
						or Neural Networks are used to recognize phonemes.</li>
<li class="segment">
<span class="segment-number">11. </span>These models are trained on
						large datasets, learning the relationships between features and phonetic symbols.</li>
<li class="segment">
<span class="segment-number">12. </span>Recent
						methods sometimes skip phoneme modeling, matching directly to words or phrases.</li>
<li class="segment">
<span class="segment-number">13. </span>Combining linguistic units to form a structured output</li>
<li class="segment">
<span class="segment-number">14. </span>After identifying phonetic or linguistic units, they are combined into a
						meaningful sentence or command.</li>
<li class="segment">
<span class="segment-number">15. </span>This step relies on language models that
						incorporate syntax and context to refine recognition.</li>
<li class="segment">
<span class="segment-number">16. </span>In some implementations,
						objects are mapped directly to nouns and actions to verbs, producing a structured
						representation.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-E-T1-1"><option value="">Select a segment</option>
<option value="1">1. Preprocessing the speech signal to enhance features for recognition</option>
<option value="2">2. This step involves techniques like noise reduction, band-pass filtering, or
						spectral subtraction.</option>
<option value="3">3. These adjustments ensure that all speech sounds are
						perceived equally by the recognition system.</option>
<option value="4">4. To handle speech rate variability,
						the signal may be normalized through time-stretching or compression.</option>
<option value="5">5. Extracting relevant acoustic features from the preprocessed speech signal</option>
<option value="6">6. Once cleaned, the signal is analyzed to extract acoustic features that help
						identify speech patterns.</option>
<option value="7">7. High-frequency components carry the most linguistic
						information, and are prioritized over low-frequency features.</option>
<option value="8">8. These features are
						typically spectral, derived from techniques such as Mel-Frequency Cepstral Coefficients
						(MFCCs) or spectrogram analysis.</option>
<option value="9">9. Using statistical models to map acoustic features to phonetic units</option>
<option value="10">10. Once features are extracted, statistical models like Hidden Markov Models (HMMs)
						or Neural Networks are used to recognize phonemes.</option>
<option value="11">11. These models are trained on
						large datasets, learning the relationships between features and phonetic symbols.</option>
<option value="12">12. Recent
						methods sometimes skip phoneme modeling, matching directly to words or phrases.</option>
<option value="13">13. Combining linguistic units to form a structured output</option>
<option value="14">14. After identifying phonetic or linguistic units, they are combined into a
						meaningful sentence or command.</option>
<option value="15">15. This step relies on language models that
						incorporate syntax and context to refine recognition.</option>
<option value="16">16. In some implementations,
						objects are mapped directly to nouns and actions to verbs, producing a structured
						representation.</option></select><textarea name="best-comment-E-T1-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-E-T1-2"><option value="">Select a segment</option>
<option value="1">1. Preprocessing the speech signal to enhance features for recognition</option>
<option value="2">2. This step involves techniques like noise reduction, band-pass filtering, or
						spectral subtraction.</option>
<option value="3">3. These adjustments ensure that all speech sounds are
						perceived equally by the recognition system.</option>
<option value="4">4. To handle speech rate variability,
						the signal may be normalized through time-stretching or compression.</option>
<option value="5">5. Extracting relevant acoustic features from the preprocessed speech signal</option>
<option value="6">6. Once cleaned, the signal is analyzed to extract acoustic features that help
						identify speech patterns.</option>
<option value="7">7. High-frequency components carry the most linguistic
						information, and are prioritized over low-frequency features.</option>
<option value="8">8. These features are
						typically spectral, derived from techniques such as Mel-Frequency Cepstral Coefficients
						(MFCCs) or spectrogram analysis.</option>
<option value="9">9. Using statistical models to map acoustic features to phonetic units</option>
<option value="10">10. Once features are extracted, statistical models like Hidden Markov Models (HMMs)
						or Neural Networks are used to recognize phonemes.</option>
<option value="11">11. These models are trained on
						large datasets, learning the relationships between features and phonetic symbols.</option>
<option value="12">12. Recent
						methods sometimes skip phoneme modeling, matching directly to words or phrases.</option>
<option value="13">13. Combining linguistic units to form a structured output</option>
<option value="14">14. After identifying phonetic or linguistic units, they are combined into a
						meaningful sentence or command.</option>
<option value="15">15. This step relies on language models that
						incorporate syntax and context to refine recognition.</option>
<option value="16">16. In some implementations,
						objects are mapped directly to nouns and actions to verbs, producing a structured
						representation.</option></select><textarea name="best-comment-E-T1-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-E-T1-1"><option value="">Select a segment</option>
<option value="1">1. Preprocessing the speech signal to enhance features for recognition</option>
<option value="2">2. This step involves techniques like noise reduction, band-pass filtering, or
						spectral subtraction.</option>
<option value="3">3. These adjustments ensure that all speech sounds are
						perceived equally by the recognition system.</option>
<option value="4">4. To handle speech rate variability,
						the signal may be normalized through time-stretching or compression.</option>
<option value="5">5. Extracting relevant acoustic features from the preprocessed speech signal</option>
<option value="6">6. Once cleaned, the signal is analyzed to extract acoustic features that help
						identify speech patterns.</option>
<option value="7">7. High-frequency components carry the most linguistic
						information, and are prioritized over low-frequency features.</option>
<option value="8">8. These features are
						typically spectral, derived from techniques such as Mel-Frequency Cepstral Coefficients
						(MFCCs) or spectrogram analysis.</option>
<option value="9">9. Using statistical models to map acoustic features to phonetic units</option>
<option value="10">10. Once features are extracted, statistical models like Hidden Markov Models (HMMs)
						or Neural Networks are used to recognize phonemes.</option>
<option value="11">11. These models are trained on
						large datasets, learning the relationships between features and phonetic symbols.</option>
<option value="12">12. Recent
						methods sometimes skip phoneme modeling, matching directly to words or phrases.</option>
<option value="13">13. Combining linguistic units to form a structured output</option>
<option value="14">14. After identifying phonetic or linguistic units, they are combined into a
						meaningful sentence or command.</option>
<option value="15">15. This step relies on language models that
						incorporate syntax and context to refine recognition.</option>
<option value="16">16. In some implementations,
						objects are mapped directly to nouns and actions to verbs, producing a structured
						representation.</option></select><textarea name="worst-comment-E-T1-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-E-T1-2"><option value="">Select a segment</option>
<option value="1">1. Preprocessing the speech signal to enhance features for recognition</option>
<option value="2">2. This step involves techniques like noise reduction, band-pass filtering, or
						spectral subtraction.</option>
<option value="3">3. These adjustments ensure that all speech sounds are
						perceived equally by the recognition system.</option>
<option value="4">4. To handle speech rate variability,
						the signal may be normalized through time-stretching or compression.</option>
<option value="5">5. Extracting relevant acoustic features from the preprocessed speech signal</option>
<option value="6">6. Once cleaned, the signal is analyzed to extract acoustic features that help
						identify speech patterns.</option>
<option value="7">7. High-frequency components carry the most linguistic
						information, and are prioritized over low-frequency features.</option>
<option value="8">8. These features are
						typically spectral, derived from techniques such as Mel-Frequency Cepstral Coefficients
						(MFCCs) or spectrogram analysis.</option>
<option value="9">9. Using statistical models to map acoustic features to phonetic units</option>
<option value="10">10. Once features are extracted, statistical models like Hidden Markov Models (HMMs)
						or Neural Networks are used to recognize phonemes.</option>
<option value="11">11. These models are trained on
						large datasets, learning the relationships between features and phonetic symbols.</option>
<option value="12">12. Recent
						methods sometimes skip phoneme modeling, matching directly to words or phrases.</option>
<option value="13">13. Combining linguistic units to form a structured output</option>
<option value="14">14. After identifying phonetic or linguistic units, they are combined into a
						meaningful sentence or command.</option>
<option value="15">15. This step relies on language models that
						incorporate syntax and context to refine recognition.</option>
<option value="16">16. In some implementations,
						objects are mapped directly to nouns and actions to verbs, producing a structured
						representation.</option></select><textarea name="worst-comment-E-T1-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>E level questions | 2 speech production and speech synthesis</h2>
<h2>Question</h2>
<p>Describe four important aspects of speech production according to the source-filter
				model.</p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>The source (vocal fold vibration)</li>
<li class="segment">
<span class="segment-number">2. </span>The sound source for speech production comes from the vibration of the vocal
						folds in the larynx.</li>
<li class="segment">
<span class="segment-number">3. </span>As air from the lungs is pushed through the vocal folds, they
						vibrate, creating a periodic waveform.</li>
<li class="segment">
<span class="segment-number">4. </span>This provides the fundamental frequency and
						its harmonics, forming the basic sound that will later be shaped by the vocal tract.</li>
<li class="segment">
<span class="segment-number">5. </span>The glottal signal</li>
<li class="segment">
<span class="segment-number">6. </span>The raw sound produced by the vibrating vocal folds is called the glottal signal.</li>
<li class="segment">
<span class="segment-number">7. </span>It
						contains energy at the fundamental frequency (the pitch of the voice) and its harmonics,
						but it is still unshaped.</li>
<li class="segment">
<span class="segment-number">8. </span>This sound is already partially filtered by the larynx
						before it reaches the vocal tract.</li>
<li class="segment">
<span class="segment-number">9. </span>The filter (vocal tract resonance)</li>
<li class="segment">
<span class="segment-number">10. </span>The vocal tract, including the throat, mouth, and nasal passages, acts as a
						resonant filter.</li>
<li class="segment">
<span class="segment-number">11. </span>The shape and position of the tongue, lips, and other
						articulators modify the sound by emphasizing certain frequencies and damping others.</li>
<li class="segment">
<span class="segment-number">12. </span>This
						filtering process affects vowels, while consonants are primarily shaped by airflow
						constrictions.</li>
<li class="segment">
<span class="segment-number">13. </span>The radiated sound (articulated speech)</li>
<li class="segment">
<span class="segment-number">14. </span>After being filtered by the vocal tract, the sound exits the mouth and, to a
						lesser extent, the nose, becoming the radiated speech.</li>
<li class="segment">
<span class="segment-number">15. </span>This final sound is the
						speech that others hear.</li>
<li class="segment">
<span class="segment-number">16. </span>The precise articulation of speech sounds—through the
						movement of the tongue, lips, and other vocal organs—refines the sound, producing the
						phonemes that make up intelligible words.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-E-T2-1"><option value="">Select a segment</option>
<option value="1">1. The source (vocal fold vibration)</option>
<option value="2">2. The sound source for speech production comes from the vibration of the vocal
						folds in the larynx.</option>
<option value="3">3. As air from the lungs is pushed through the vocal folds, they
						vibrate, creating a periodic waveform.</option>
<option value="4">4. This provides the fundamental frequency and
						its harmonics, forming the basic sound that will later be shaped by the vocal tract.</option>
<option value="5">5. The glottal signal</option>
<option value="6">6. The raw sound produced by the vibrating vocal folds is called the glottal signal.</option>
<option value="7">7. It
						contains energy at the fundamental frequency (the pitch of the voice) and its harmonics,
						but it is still unshaped.</option>
<option value="8">8. This sound is already partially filtered by the larynx
						before it reaches the vocal tract.</option>
<option value="9">9. The filter (vocal tract resonance)</option>
<option value="10">10. The vocal tract, including the throat, mouth, and nasal passages, acts as a
						resonant filter.</option>
<option value="11">11. The shape and position of the tongue, lips, and other
						articulators modify the sound by emphasizing certain frequencies and damping others.</option>
<option value="12">12. This
						filtering process affects vowels, while consonants are primarily shaped by airflow
						constrictions.</option>
<option value="13">13. The radiated sound (articulated speech)</option>
<option value="14">14. After being filtered by the vocal tract, the sound exits the mouth and, to a
						lesser extent, the nose, becoming the radiated speech.</option>
<option value="15">15. This final sound is the
						speech that others hear.</option>
<option value="16">16. The precise articulation of speech sounds—through the
						movement of the tongue, lips, and other vocal organs—refines the sound, producing the
						phonemes that make up intelligible words.</option></select><textarea name="best-comment-E-T2-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-E-T2-2"><option value="">Select a segment</option>
<option value="1">1. The source (vocal fold vibration)</option>
<option value="2">2. The sound source for speech production comes from the vibration of the vocal
						folds in the larynx.</option>
<option value="3">3. As air from the lungs is pushed through the vocal folds, they
						vibrate, creating a periodic waveform.</option>
<option value="4">4. This provides the fundamental frequency and
						its harmonics, forming the basic sound that will later be shaped by the vocal tract.</option>
<option value="5">5. The glottal signal</option>
<option value="6">6. The raw sound produced by the vibrating vocal folds is called the glottal signal.</option>
<option value="7">7. It
						contains energy at the fundamental frequency (the pitch of the voice) and its harmonics,
						but it is still unshaped.</option>
<option value="8">8. This sound is already partially filtered by the larynx
						before it reaches the vocal tract.</option>
<option value="9">9. The filter (vocal tract resonance)</option>
<option value="10">10. The vocal tract, including the throat, mouth, and nasal passages, acts as a
						resonant filter.</option>
<option value="11">11. The shape and position of the tongue, lips, and other
						articulators modify the sound by emphasizing certain frequencies and damping others.</option>
<option value="12">12. This
						filtering process affects vowels, while consonants are primarily shaped by airflow
						constrictions.</option>
<option value="13">13. The radiated sound (articulated speech)</option>
<option value="14">14. After being filtered by the vocal tract, the sound exits the mouth and, to a
						lesser extent, the nose, becoming the radiated speech.</option>
<option value="15">15. This final sound is the
						speech that others hear.</option>
<option value="16">16. The precise articulation of speech sounds—through the
						movement of the tongue, lips, and other vocal organs—refines the sound, producing the
						phonemes that make up intelligible words.</option></select><textarea name="best-comment-E-T2-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-E-T2-1"><option value="">Select a segment</option>
<option value="1">1. The source (vocal fold vibration)</option>
<option value="2">2. The sound source for speech production comes from the vibration of the vocal
						folds in the larynx.</option>
<option value="3">3. As air from the lungs is pushed through the vocal folds, they
						vibrate, creating a periodic waveform.</option>
<option value="4">4. This provides the fundamental frequency and
						its harmonics, forming the basic sound that will later be shaped by the vocal tract.</option>
<option value="5">5. The glottal signal</option>
<option value="6">6. The raw sound produced by the vibrating vocal folds is called the glottal signal.</option>
<option value="7">7. It
						contains energy at the fundamental frequency (the pitch of the voice) and its harmonics,
						but it is still unshaped.</option>
<option value="8">8. This sound is already partially filtered by the larynx
						before it reaches the vocal tract.</option>
<option value="9">9. The filter (vocal tract resonance)</option>
<option value="10">10. The vocal tract, including the throat, mouth, and nasal passages, acts as a
						resonant filter.</option>
<option value="11">11. The shape and position of the tongue, lips, and other
						articulators modify the sound by emphasizing certain frequencies and damping others.</option>
<option value="12">12. This
						filtering process affects vowels, while consonants are primarily shaped by airflow
						constrictions.</option>
<option value="13">13. The radiated sound (articulated speech)</option>
<option value="14">14. After being filtered by the vocal tract, the sound exits the mouth and, to a
						lesser extent, the nose, becoming the radiated speech.</option>
<option value="15">15. This final sound is the
						speech that others hear.</option>
<option value="16">16. The precise articulation of speech sounds—through the
						movement of the tongue, lips, and other vocal organs—refines the sound, producing the
						phonemes that make up intelligible words.</option></select><textarea name="worst-comment-E-T2-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-E-T2-2"><option value="">Select a segment</option>
<option value="1">1. The source (vocal fold vibration)</option>
<option value="2">2. The sound source for speech production comes from the vibration of the vocal
						folds in the larynx.</option>
<option value="3">3. As air from the lungs is pushed through the vocal folds, they
						vibrate, creating a periodic waveform.</option>
<option value="4">4. This provides the fundamental frequency and
						its harmonics, forming the basic sound that will later be shaped by the vocal tract.</option>
<option value="5">5. The glottal signal</option>
<option value="6">6. The raw sound produced by the vibrating vocal folds is called the glottal signal.</option>
<option value="7">7. It
						contains energy at the fundamental frequency (the pitch of the voice) and its harmonics,
						but it is still unshaped.</option>
<option value="8">8. This sound is already partially filtered by the larynx
						before it reaches the vocal tract.</option>
<option value="9">9. The filter (vocal tract resonance)</option>
<option value="10">10. The vocal tract, including the throat, mouth, and nasal passages, acts as a
						resonant filter.</option>
<option value="11">11. The shape and position of the tongue, lips, and other
						articulators modify the sound by emphasizing certain frequencies and damping others.</option>
<option value="12">12. This
						filtering process affects vowels, while consonants are primarily shaped by airflow
						constrictions.</option>
<option value="13">13. The radiated sound (articulated speech)</option>
<option value="14">14. After being filtered by the vocal tract, the sound exits the mouth and, to a
						lesser extent, the nose, becoming the radiated speech.</option>
<option value="15">15. This final sound is the
						speech that others hear.</option>
<option value="16">16. The precise articulation of speech sounds—through the
						movement of the tongue, lips, and other vocal organs—refines the sound, producing the
						phonemes that make up intelligible words.</option></select><textarea name="worst-comment-E-T2-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>E level questions | 3 dialogue systems</h2>
<h2>Question</h2>
<p> Describe the key components of a typical task-oriented dialogue system. Explain the
				function of each component and how they interact to process a user query from input to
				response generation. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>A typical task-oriented dialogue system consists of the following components:</li>
<li class="segment">
<span class="segment-number">2. </span>Automatic speech recognition (ASR)</li>
<li class="segment">
<span class="segment-number">3. </span>Converts spoken input into text.</li>
<li class="segment">
<span class="segment-number">4. </span>It uses models like Hidden Markov Models
						(HMMs) or deep neural networks (DNNs).</li>
<li class="segment">
<span class="segment-number">5. </span>ASR does not perform semantic analysis; it
						only transcribes speech into text.</li>
<li class="segment">
<span class="segment-number">6. </span>Intent recognition is handled by NLU.</li>
<li class="segment">
<span class="segment-number">7. </span>Natural language understanding (NLU)</li>
<li class="segment">
<span class="segment-number">8. </span>Extracts meaning from the transcribed text, including intent recognition and
						entity extraction.</li>
<li class="segment">
<span class="segment-number">9. </span>ASR also applies semantic analysis to extract intent directly
						from the speech signal.</li>
<li class="segment">
<span class="segment-number">10. </span>Dialogue manager (DM)</li>
<li class="segment">
<span class="segment-number">11. </span>Maintains context, tracks the conversation state, and decides the system’s next
						action based on user input and system goals.</li>
<li class="segment">
<span class="segment-number">12. </span>The DM primarily selects responses
						from a fixed script without adapting dynamically to conversation context.</li>
<li class="segment">
<span class="segment-number">13. </span>Natural language generation (NLG)</li>
<li class="segment">
<span class="segment-number">14. </span>Converts the system’s response into natural, human-like text.</li>
<li class="segment">
<span class="segment-number">15. </span>Text-to-Speech (TTS) synthesis (if applicable)</li>
<li class="segment">
<span class="segment-number">16. </span>Converts the generated text into spoken output.</li>
<li class="segment">
<span class="segment-number">17. </span>Knowledge base / external APIs (if applicable)</li>
<li class="segment">
<span class="segment-number">18. </span>A database or external service that provides information to fulfill the user’s
						request.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-E-T3-1"><option value="">Select a segment</option>
<option value="1">1. A typical task-oriented dialogue system consists of the following components:</option>
<option value="2">2. Automatic speech recognition (ASR)</option>
<option value="3">3. Converts spoken input into text.</option>
<option value="4">4. It uses models like Hidden Markov Models
						(HMMs) or deep neural networks (DNNs).</option>
<option value="5">5. ASR does not perform semantic analysis; it
						only transcribes speech into text.</option>
<option value="6">6. Intent recognition is handled by NLU.</option>
<option value="7">7. Natural language understanding (NLU)</option>
<option value="8">8. Extracts meaning from the transcribed text, including intent recognition and
						entity extraction.</option>
<option value="9">9. ASR also applies semantic analysis to extract intent directly
						from the speech signal.</option>
<option value="10">10. Dialogue manager (DM)</option>
<option value="11">11. Maintains context, tracks the conversation state, and decides the system’s next
						action based on user input and system goals.</option>
<option value="12">12. The DM primarily selects responses
						from a fixed script without adapting dynamically to conversation context.</option>
<option value="13">13. Natural language generation (NLG)</option>
<option value="14">14. Converts the system’s response into natural, human-like text.</option>
<option value="15">15. Text-to-Speech (TTS) synthesis (if applicable)</option>
<option value="16">16. Converts the generated text into spoken output.</option>
<option value="17">17. Knowledge base / external APIs (if applicable)</option>
<option value="18">18. A database or external service that provides information to fulfill the user’s
						request.</option></select><textarea name="best-comment-E-T3-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-E-T3-2"><option value="">Select a segment</option>
<option value="1">1. A typical task-oriented dialogue system consists of the following components:</option>
<option value="2">2. Automatic speech recognition (ASR)</option>
<option value="3">3. Converts spoken input into text.</option>
<option value="4">4. It uses models like Hidden Markov Models
						(HMMs) or deep neural networks (DNNs).</option>
<option value="5">5. ASR does not perform semantic analysis; it
						only transcribes speech into text.</option>
<option value="6">6. Intent recognition is handled by NLU.</option>
<option value="7">7. Natural language understanding (NLU)</option>
<option value="8">8. Extracts meaning from the transcribed text, including intent recognition and
						entity extraction.</option>
<option value="9">9. ASR also applies semantic analysis to extract intent directly
						from the speech signal.</option>
<option value="10">10. Dialogue manager (DM)</option>
<option value="11">11. Maintains context, tracks the conversation state, and decides the system’s next
						action based on user input and system goals.</option>
<option value="12">12. The DM primarily selects responses
						from a fixed script without adapting dynamically to conversation context.</option>
<option value="13">13. Natural language generation (NLG)</option>
<option value="14">14. Converts the system’s response into natural, human-like text.</option>
<option value="15">15. Text-to-Speech (TTS) synthesis (if applicable)</option>
<option value="16">16. Converts the generated text into spoken output.</option>
<option value="17">17. Knowledge base / external APIs (if applicable)</option>
<option value="18">18. A database or external service that provides information to fulfill the user’s
						request.</option></select><textarea name="best-comment-E-T3-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-E-T3-1"><option value="">Select a segment</option>
<option value="1">1. A typical task-oriented dialogue system consists of the following components:</option>
<option value="2">2. Automatic speech recognition (ASR)</option>
<option value="3">3. Converts spoken input into text.</option>
<option value="4">4. It uses models like Hidden Markov Models
						(HMMs) or deep neural networks (DNNs).</option>
<option value="5">5. ASR does not perform semantic analysis; it
						only transcribes speech into text.</option>
<option value="6">6. Intent recognition is handled by NLU.</option>
<option value="7">7. Natural language understanding (NLU)</option>
<option value="8">8. Extracts meaning from the transcribed text, including intent recognition and
						entity extraction.</option>
<option value="9">9. ASR also applies semantic analysis to extract intent directly
						from the speech signal.</option>
<option value="10">10. Dialogue manager (DM)</option>
<option value="11">11. Maintains context, tracks the conversation state, and decides the system’s next
						action based on user input and system goals.</option>
<option value="12">12. The DM primarily selects responses
						from a fixed script without adapting dynamically to conversation context.</option>
<option value="13">13. Natural language generation (NLG)</option>
<option value="14">14. Converts the system’s response into natural, human-like text.</option>
<option value="15">15. Text-to-Speech (TTS) synthesis (if applicable)</option>
<option value="16">16. Converts the generated text into spoken output.</option>
<option value="17">17. Knowledge base / external APIs (if applicable)</option>
<option value="18">18. A database or external service that provides information to fulfill the user’s
						request.</option></select><textarea name="worst-comment-E-T3-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-E-T3-2"><option value="">Select a segment</option>
<option value="1">1. A typical task-oriented dialogue system consists of the following components:</option>
<option value="2">2. Automatic speech recognition (ASR)</option>
<option value="3">3. Converts spoken input into text.</option>
<option value="4">4. It uses models like Hidden Markov Models
						(HMMs) or deep neural networks (DNNs).</option>
<option value="5">5. ASR does not perform semantic analysis; it
						only transcribes speech into text.</option>
<option value="6">6. Intent recognition is handled by NLU.</option>
<option value="7">7. Natural language understanding (NLU)</option>
<option value="8">8. Extracts meaning from the transcribed text, including intent recognition and
						entity extraction.</option>
<option value="9">9. ASR also applies semantic analysis to extract intent directly
						from the speech signal.</option>
<option value="10">10. Dialogue manager (DM)</option>
<option value="11">11. Maintains context, tracks the conversation state, and decides the system’s next
						action based on user input and system goals.</option>
<option value="12">12. The DM primarily selects responses
						from a fixed script without adapting dynamically to conversation context.</option>
<option value="13">13. Natural language generation (NLG)</option>
<option value="14">14. Converts the system’s response into natural, human-like text.</option>
<option value="15">15. Text-to-Speech (TTS) synthesis (if applicable)</option>
<option value="16">16. Converts the generated text into spoken output.</option>
<option value="17">17. Knowledge base / external APIs (if applicable)</option>
<option value="18">18. A database or external service that provides information to fulfill the user’s
						request.</option></select><textarea name="worst-comment-E-T3-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>E level questions | 4 data collection</h2>
<h2>Question</h2>
<p> When collecting dialogue data for training a conversational AI system, several
				challenges must be addressed. Identify and explain four key challenges in dialogue data
				collection. For each challenge, suggest a possible solution or mitigation strategy,
				providing examples where relevant. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Data privacy and ethical concerns</li>
<li class="segment">
<span class="segment-number">2. </span>When collecting conversational data, especially from real users, ensuring privacy
						and ethical compliance is crucial.</li>
<li class="segment">
<span class="segment-number">3. </span>Example: If recording customer service calls,
						companies must anonymize sensitive data and comply with GDPR or other regulations.</li>
<li class="segment">
<span class="segment-number">4. </span>Solution:
						Obtain user consent, anonymize personally identifiable information (PII), and follow
						ethical AI guidelines.</li>
<li class="segment">
<span class="segment-number">5. </span>However, once data is anonymized, it no longer poses any
						ethical risks.</li>
<li class="segment">
<span class="segment-number">6. </span>Data diversity and representativeness</li>
<li class="segment">
<span class="segment-number">7. </span>A dataset should reflect diverse user demographics, languages, and conversation
						styles to prevent bias in the system.</li>
<li class="segment">
<span class="segment-number">8. </span>Example: If a chatbot is trained only on
						English conversations from young users, it may struggle with older users or those
						speaking with an accent.</li>
<li class="segment">
<span class="segment-number">9. </span>Solution: Collect data from diverse sources, including
						different age groups, accents, and communication styles.</li>
<li class="segment">
<span class="segment-number">10. </span>If a dataset is large
						enough, diversity concerns become negligible.</li>
<li class="segment">
<span class="segment-number">11. </span>Data annotation and quality control</li>
<li class="segment">
<span class="segment-number">12. </span>Dialogue data needs accurate labeling (e.g., intents, slots, or dialogue acts),
						but annotation can be subjective and inconsistent.</li>
<li class="segment">
<span class="segment-number">13. </span>Example: Two annotators may
						label the same user query differently, affecting training consistency.</li>
<li class="segment">
<span class="segment-number">14. </span>Solution:
						Use multiple annotators, establish clear guidelines, and calculate inter-annotator
						agreement to ensure quality.</li>
<li class="segment">
<span class="segment-number">15. </span>Conversational context and long-term dependencies</li>
<li class="segment">
<span class="segment-number">16. </span>Dialogue systems often need to handle multi-turn interactions and remember past
						exchanges for meaningful responses.</li>
<li class="segment">
<span class="segment-number">17. </span>Example: A customer support chatbot should
						recall a user's previous questions within a session to avoid repetition.</li>
<li class="segment">
<span class="segment-number">18. </span>Solution:
						Use memory-aware architectures, such as transformers with attention mechanisms, or
						maintain session-based context tracking to improve continuity in conversations.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-E-T4-1"><option value="">Select a segment</option>
<option value="1">1. Data privacy and ethical concerns</option>
<option value="2">2. When collecting conversational data, especially from real users, ensuring privacy
						and ethical compliance is crucial.</option>
<option value="3">3. Example: If recording customer service calls,
						companies must anonymize sensitive data and comply with GDPR or other regulations.</option>
<option value="4">4. Solution:
						Obtain user consent, anonymize personally identifiable information (PII), and follow
						ethical AI guidelines.</option>
<option value="5">5. However, once data is anonymized, it no longer poses any
						ethical risks.</option>
<option value="6">6. Data diversity and representativeness</option>
<option value="7">7. A dataset should reflect diverse user demographics, languages, and conversation
						styles to prevent bias in the system.</option>
<option value="8">8. Example: If a chatbot is trained only on
						English conversations from young users, it may struggle with older users or those
						speaking with an accent.</option>
<option value="9">9. Solution: Collect data from diverse sources, including
						different age groups, accents, and communication styles.</option>
<option value="10">10. If a dataset is large
						enough, diversity concerns become negligible.</option>
<option value="11">11. Data annotation and quality control</option>
<option value="12">12. Dialogue data needs accurate labeling (e.g., intents, slots, or dialogue acts),
						but annotation can be subjective and inconsistent.</option>
<option value="13">13. Example: Two annotators may
						label the same user query differently, affecting training consistency.</option>
<option value="14">14. Solution:
						Use multiple annotators, establish clear guidelines, and calculate inter-annotator
						agreement to ensure quality.</option>
<option value="15">15. Conversational context and long-term dependencies</option>
<option value="16">16. Dialogue systems often need to handle multi-turn interactions and remember past
						exchanges for meaningful responses.</option>
<option value="17">17. Example: A customer support chatbot should
						recall a user's previous questions within a session to avoid repetition.</option>
<option value="18">18. Solution:
						Use memory-aware architectures, such as transformers with attention mechanisms, or
						maintain session-based context tracking to improve continuity in conversations.</option></select><textarea name="best-comment-E-T4-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-E-T4-2"><option value="">Select a segment</option>
<option value="1">1. Data privacy and ethical concerns</option>
<option value="2">2. When collecting conversational data, especially from real users, ensuring privacy
						and ethical compliance is crucial.</option>
<option value="3">3. Example: If recording customer service calls,
						companies must anonymize sensitive data and comply with GDPR or other regulations.</option>
<option value="4">4. Solution:
						Obtain user consent, anonymize personally identifiable information (PII), and follow
						ethical AI guidelines.</option>
<option value="5">5. However, once data is anonymized, it no longer poses any
						ethical risks.</option>
<option value="6">6. Data diversity and representativeness</option>
<option value="7">7. A dataset should reflect diverse user demographics, languages, and conversation
						styles to prevent bias in the system.</option>
<option value="8">8. Example: If a chatbot is trained only on
						English conversations from young users, it may struggle with older users or those
						speaking with an accent.</option>
<option value="9">9. Solution: Collect data from diverse sources, including
						different age groups, accents, and communication styles.</option>
<option value="10">10. If a dataset is large
						enough, diversity concerns become negligible.</option>
<option value="11">11. Data annotation and quality control</option>
<option value="12">12. Dialogue data needs accurate labeling (e.g., intents, slots, or dialogue acts),
						but annotation can be subjective and inconsistent.</option>
<option value="13">13. Example: Two annotators may
						label the same user query differently, affecting training consistency.</option>
<option value="14">14. Solution:
						Use multiple annotators, establish clear guidelines, and calculate inter-annotator
						agreement to ensure quality.</option>
<option value="15">15. Conversational context and long-term dependencies</option>
<option value="16">16. Dialogue systems often need to handle multi-turn interactions and remember past
						exchanges for meaningful responses.</option>
<option value="17">17. Example: A customer support chatbot should
						recall a user's previous questions within a session to avoid repetition.</option>
<option value="18">18. Solution:
						Use memory-aware architectures, such as transformers with attention mechanisms, or
						maintain session-based context tracking to improve continuity in conversations.</option></select><textarea name="best-comment-E-T4-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-E-T4-1"><option value="">Select a segment</option>
<option value="1">1. Data privacy and ethical concerns</option>
<option value="2">2. When collecting conversational data, especially from real users, ensuring privacy
						and ethical compliance is crucial.</option>
<option value="3">3. Example: If recording customer service calls,
						companies must anonymize sensitive data and comply with GDPR or other regulations.</option>
<option value="4">4. Solution:
						Obtain user consent, anonymize personally identifiable information (PII), and follow
						ethical AI guidelines.</option>
<option value="5">5. However, once data is anonymized, it no longer poses any
						ethical risks.</option>
<option value="6">6. Data diversity and representativeness</option>
<option value="7">7. A dataset should reflect diverse user demographics, languages, and conversation
						styles to prevent bias in the system.</option>
<option value="8">8. Example: If a chatbot is trained only on
						English conversations from young users, it may struggle with older users or those
						speaking with an accent.</option>
<option value="9">9. Solution: Collect data from diverse sources, including
						different age groups, accents, and communication styles.</option>
<option value="10">10. If a dataset is large
						enough, diversity concerns become negligible.</option>
<option value="11">11. Data annotation and quality control</option>
<option value="12">12. Dialogue data needs accurate labeling (e.g., intents, slots, or dialogue acts),
						but annotation can be subjective and inconsistent.</option>
<option value="13">13. Example: Two annotators may
						label the same user query differently, affecting training consistency.</option>
<option value="14">14. Solution:
						Use multiple annotators, establish clear guidelines, and calculate inter-annotator
						agreement to ensure quality.</option>
<option value="15">15. Conversational context and long-term dependencies</option>
<option value="16">16. Dialogue systems often need to handle multi-turn interactions and remember past
						exchanges for meaningful responses.</option>
<option value="17">17. Example: A customer support chatbot should
						recall a user's previous questions within a session to avoid repetition.</option>
<option value="18">18. Solution:
						Use memory-aware architectures, such as transformers with attention mechanisms, or
						maintain session-based context tracking to improve continuity in conversations.</option></select><textarea name="worst-comment-E-T4-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-E-T4-2"><option value="">Select a segment</option>
<option value="1">1. Data privacy and ethical concerns</option>
<option value="2">2. When collecting conversational data, especially from real users, ensuring privacy
						and ethical compliance is crucial.</option>
<option value="3">3. Example: If recording customer service calls,
						companies must anonymize sensitive data and comply with GDPR or other regulations.</option>
<option value="4">4. Solution:
						Obtain user consent, anonymize personally identifiable information (PII), and follow
						ethical AI guidelines.</option>
<option value="5">5. However, once data is anonymized, it no longer poses any
						ethical risks.</option>
<option value="6">6. Data diversity and representativeness</option>
<option value="7">7. A dataset should reflect diverse user demographics, languages, and conversation
						styles to prevent bias in the system.</option>
<option value="8">8. Example: If a chatbot is trained only on
						English conversations from young users, it may struggle with older users or those
						speaking with an accent.</option>
<option value="9">9. Solution: Collect data from diverse sources, including
						different age groups, accents, and communication styles.</option>
<option value="10">10. If a dataset is large
						enough, diversity concerns become negligible.</option>
<option value="11">11. Data annotation and quality control</option>
<option value="12">12. Dialogue data needs accurate labeling (e.g., intents, slots, or dialogue acts),
						but annotation can be subjective and inconsistent.</option>
<option value="13">13. Example: Two annotators may
						label the same user query differently, affecting training consistency.</option>
<option value="14">14. Solution:
						Use multiple annotators, establish clear guidelines, and calculate inter-annotator
						agreement to ensure quality.</option>
<option value="15">15. Conversational context and long-term dependencies</option>
<option value="16">16. Dialogue systems often need to handle multi-turn interactions and remember past
						exchanges for meaningful responses.</option>
<option value="17">17. Example: A customer support chatbot should
						recall a user's previous questions within a session to avoid repetition.</option>
<option value="18">18. Solution:
						Use memory-aware architectures, such as transformers with attention mechanisms, or
						maintain session-based context tracking to improve continuity in conversations.</option></select><textarea name="worst-comment-E-T4-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>E level questions | 5 evaluation</h2>
<h2>Question</h2>
<p></p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Define the evaluation metrics</li>
<li class="segment">
<span class="segment-number">2. </span>It's important to clearly define the metrics that will be used to evaluate the
						performance of the speech technology, such as accuracy, precision, and recall.</li>
<li class="segment">
<span class="segment-number">3. </span>For
						subjective evaluations, relying purely on automated scores trained on human judgements
						ensures an unbiased assessment.</li>
<li class="segment">
<span class="segment-number">4. </span>However, combining objective and subjective
						assessments provides a more comprehensive evaluation.</li>
<li class="segment">
<span class="segment-number">5. </span>Define the goal of the evaluation</li>
<li class="segment">
<span class="segment-number">6. </span>The evaluation should be designed to measure how well a system performs under
						ideal conditions before testing it on real-world data.</li>
<li class="segment">
<span class="segment-number">7. </span>Once the metrics are in
						place, it is important to clearly state what is to be evaluated.</li>
<li class="segment">
<span class="segment-number">8. </span>Real-world
						testing is essential early on, as systems behave differently in varied acoustic and user
						conditions.</li>
<li class="segment">
<span class="segment-number">9. </span>Select the evaluation data</li>
<li class="segment">
<span class="segment-number">10. </span>The evaluation data should be representative of the type of data that the speech
						technology will encounter in real-world applications.</li>
<li class="segment">
<span class="segment-number">11. </span>This can include speech
						samples from a specific domain or genre, or a mix of different types of speech data.</li>
<li class="segment">
<span class="segment-number">12. </span>Consider the human factor</li>
<li class="segment">
<span class="segment-number">13. </span>Humans play a key role in evaluating speech technology, particularly in assessing
						user experience and naturalness.</li>
<li class="segment">
<span class="segment-number">14. </span>For example, different users may perceive
						synthesized speech quality differently based on their linguistic background or
						familiarity with the system.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-E-T5-1"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. It's important to clearly define the metrics that will be used to evaluate the
						performance of the speech technology, such as accuracy, precision, and recall.</option>
<option value="3">3. For
						subjective evaluations, relying purely on automated scores trained on human judgements
						ensures an unbiased assessment.</option>
<option value="4">4. However, combining objective and subjective
						assessments provides a more comprehensive evaluation.</option>
<option value="5">5. Define the goal of the evaluation</option>
<option value="6">6. The evaluation should be designed to measure how well a system performs under
						ideal conditions before testing it on real-world data.</option>
<option value="7">7. Once the metrics are in
						place, it is important to clearly state what is to be evaluated.</option>
<option value="8">8. Real-world
						testing is essential early on, as systems behave differently in varied acoustic and user
						conditions.</option>
<option value="9">9. Select the evaluation data</option>
<option value="10">10. The evaluation data should be representative of the type of data that the speech
						technology will encounter in real-world applications.</option>
<option value="11">11. This can include speech
						samples from a specific domain or genre, or a mix of different types of speech data.</option>
<option value="12">12. Consider the human factor</option>
<option value="13">13. Humans play a key role in evaluating speech technology, particularly in assessing
						user experience and naturalness.</option>
<option value="14">14. For example, different users may perceive
						synthesized speech quality differently based on their linguistic background or
						familiarity with the system.</option></select><textarea name="best-comment-E-T5-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-E-T5-2"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. It's important to clearly define the metrics that will be used to evaluate the
						performance of the speech technology, such as accuracy, precision, and recall.</option>
<option value="3">3. For
						subjective evaluations, relying purely on automated scores trained on human judgements
						ensures an unbiased assessment.</option>
<option value="4">4. However, combining objective and subjective
						assessments provides a more comprehensive evaluation.</option>
<option value="5">5. Define the goal of the evaluation</option>
<option value="6">6. The evaluation should be designed to measure how well a system performs under
						ideal conditions before testing it on real-world data.</option>
<option value="7">7. Once the metrics are in
						place, it is important to clearly state what is to be evaluated.</option>
<option value="8">8. Real-world
						testing is essential early on, as systems behave differently in varied acoustic and user
						conditions.</option>
<option value="9">9. Select the evaluation data</option>
<option value="10">10. The evaluation data should be representative of the type of data that the speech
						technology will encounter in real-world applications.</option>
<option value="11">11. This can include speech
						samples from a specific domain or genre, or a mix of different types of speech data.</option>
<option value="12">12. Consider the human factor</option>
<option value="13">13. Humans play a key role in evaluating speech technology, particularly in assessing
						user experience and naturalness.</option>
<option value="14">14. For example, different users may perceive
						synthesized speech quality differently based on their linguistic background or
						familiarity with the system.</option></select><textarea name="best-comment-E-T5-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-E-T5-1"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. It's important to clearly define the metrics that will be used to evaluate the
						performance of the speech technology, such as accuracy, precision, and recall.</option>
<option value="3">3. For
						subjective evaluations, relying purely on automated scores trained on human judgements
						ensures an unbiased assessment.</option>
<option value="4">4. However, combining objective and subjective
						assessments provides a more comprehensive evaluation.</option>
<option value="5">5. Define the goal of the evaluation</option>
<option value="6">6. The evaluation should be designed to measure how well a system performs under
						ideal conditions before testing it on real-world data.</option>
<option value="7">7. Once the metrics are in
						place, it is important to clearly state what is to be evaluated.</option>
<option value="8">8. Real-world
						testing is essential early on, as systems behave differently in varied acoustic and user
						conditions.</option>
<option value="9">9. Select the evaluation data</option>
<option value="10">10. The evaluation data should be representative of the type of data that the speech
						technology will encounter in real-world applications.</option>
<option value="11">11. This can include speech
						samples from a specific domain or genre, or a mix of different types of speech data.</option>
<option value="12">12. Consider the human factor</option>
<option value="13">13. Humans play a key role in evaluating speech technology, particularly in assessing
						user experience and naturalness.</option>
<option value="14">14. For example, different users may perceive
						synthesized speech quality differently based on their linguistic background or
						familiarity with the system.</option></select><textarea name="worst-comment-E-T5-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-E-T5-2"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. It's important to clearly define the metrics that will be used to evaluate the
						performance of the speech technology, such as accuracy, precision, and recall.</option>
<option value="3">3. For
						subjective evaluations, relying purely on automated scores trained on human judgements
						ensures an unbiased assessment.</option>
<option value="4">4. However, combining objective and subjective
						assessments provides a more comprehensive evaluation.</option>
<option value="5">5. Define the goal of the evaluation</option>
<option value="6">6. The evaluation should be designed to measure how well a system performs under
						ideal conditions before testing it on real-world data.</option>
<option value="7">7. Once the metrics are in
						place, it is important to clearly state what is to be evaluated.</option>
<option value="8">8. Real-world
						testing is essential early on, as systems behave differently in varied acoustic and user
						conditions.</option>
<option value="9">9. Select the evaluation data</option>
<option value="10">10. The evaluation data should be representative of the type of data that the speech
						technology will encounter in real-world applications.</option>
<option value="11">11. This can include speech
						samples from a specific domain or genre, or a mix of different types of speech data.</option>
<option value="12">12. Consider the human factor</option>
<option value="13">13. Humans play a key role in evaluating speech technology, particularly in assessing
						user experience and naturalness.</option>
<option value="14">14. For example, different users may perceive
						synthesized speech quality differently based on their linguistic background or
						familiarity with the system.</option></select><textarea name="worst-comment-E-T5-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>C level questions | 1 perception and speech recognition</h2>
<h2>Question</h2>
<p>Describe four steps of human speech perception and relate each step to an analogous
				aspect of ASR. Note any important similarities and differences between human speech
				perception and ASR.</p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Acoustic signal processing</li>
<li class="segment">
<span class="segment-number">2. </span>Humans convert sound waves into neural signals through the cochlea and auditory
						nerve, naturally filtering noise and adapting to variations in speech.</li>
<li class="segment">
<span class="segment-number">3. </span>ASR systems
						use microphones to capture audio, then apply digital signal processing (e.g., FFT,
						MFCCs) to extract relevant features.</li>
<li class="segment">
<span class="segment-number">4. </span>Both transform raw sound into a structured
						representation, but humans depend entirely on passive hearing mechanisms, while ASR can
						selectively focus on different frequency bands.</li>
<li class="segment">
<span class="segment-number">5. </span>Phonetic and phonological analysis</li>
<li class="segment">
<span class="segment-number">6. </span>Humans segment speech into phonemes based on auditory input, considering
						coarticulation and context.</li>
<li class="segment">
<span class="segment-number">7. </span>ASR systems analyze phonemes holistically, considering
						full words before identifying individual phonemes.</li>
<li class="segment">
<span class="segment-number">8. </span>Both transform raw sound into a
						structured representation, but humans depend entirely on passive hearing mechanisms,
						while ASR can selectively focus on different frequency bands.</li>
<li class="segment">
<span class="segment-number">9. </span>Lexical and syntactic processing</li>
<li class="segment">
<span class="segment-number">10. </span>Humans use grammar and context to form meaningful words and sentences based on
						rules.</li>
<li class="segment">
<span class="segment-number">11. </span>Speech recognition systems employ probabilistic models to predict likely
						word sequences.</li>
<li class="segment">
<span class="segment-number">12. </span>ASR systems analyze phonemes holistically, considering full words
						before identifying individual phonemes.</li>
<li class="segment">
<span class="segment-number">13. </span>Semantic and pragmatic understanding</li>
<li class="segment">
<span class="segment-number">14. </span>Humans interpret intent, tone, and meaning by integrating linguistic input with
						prior knowledge, social cues, and discourse context.</li>
<li class="segment">
<span class="segment-number">15. </span>Speech recognition systems
						also rely on contextual modeling, but may struggle with implicit meanings, sarcasm, or
						ambiguous phrasing.</li>
<li class="segment">
<span class="segment-number">16. </span>Both transform raw sound into a structured representation, but
						humans depend entirely on passive hearing mechanisms, while ASR can selectively focus on
						different frequency bands.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-C-T1-1"><option value="">Select a segment</option>
<option value="1">1. Acoustic signal processing</option>
<option value="2">2. Humans convert sound waves into neural signals through the cochlea and auditory
						nerve, naturally filtering noise and adapting to variations in speech.</option>
<option value="3">3. ASR systems
						use microphones to capture audio, then apply digital signal processing (e.g., FFT,
						MFCCs) to extract relevant features.</option>
<option value="4">4. Both transform raw sound into a structured
						representation, but humans depend entirely on passive hearing mechanisms, while ASR can
						selectively focus on different frequency bands.</option>
<option value="5">5. Phonetic and phonological analysis</option>
<option value="6">6. Humans segment speech into phonemes based on auditory input, considering
						coarticulation and context.</option>
<option value="7">7. ASR systems analyze phonemes holistically, considering
						full words before identifying individual phonemes.</option>
<option value="8">8. Both transform raw sound into a
						structured representation, but humans depend entirely on passive hearing mechanisms,
						while ASR can selectively focus on different frequency bands.</option>
<option value="9">9. Lexical and syntactic processing</option>
<option value="10">10. Humans use grammar and context to form meaningful words and sentences based on
						rules.</option>
<option value="11">11. Speech recognition systems employ probabilistic models to predict likely
						word sequences.</option>
<option value="12">12. ASR systems analyze phonemes holistically, considering full words
						before identifying individual phonemes.</option>
<option value="13">13. Semantic and pragmatic understanding</option>
<option value="14">14. Humans interpret intent, tone, and meaning by integrating linguistic input with
						prior knowledge, social cues, and discourse context.</option>
<option value="15">15. Speech recognition systems
						also rely on contextual modeling, but may struggle with implicit meanings, sarcasm, or
						ambiguous phrasing.</option>
<option value="16">16. Both transform raw sound into a structured representation, but
						humans depend entirely on passive hearing mechanisms, while ASR can selectively focus on
						different frequency bands.</option></select><textarea name="best-comment-C-T1-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-C-T1-2"><option value="">Select a segment</option>
<option value="1">1. Acoustic signal processing</option>
<option value="2">2. Humans convert sound waves into neural signals through the cochlea and auditory
						nerve, naturally filtering noise and adapting to variations in speech.</option>
<option value="3">3. ASR systems
						use microphones to capture audio, then apply digital signal processing (e.g., FFT,
						MFCCs) to extract relevant features.</option>
<option value="4">4. Both transform raw sound into a structured
						representation, but humans depend entirely on passive hearing mechanisms, while ASR can
						selectively focus on different frequency bands.</option>
<option value="5">5. Phonetic and phonological analysis</option>
<option value="6">6. Humans segment speech into phonemes based on auditory input, considering
						coarticulation and context.</option>
<option value="7">7. ASR systems analyze phonemes holistically, considering
						full words before identifying individual phonemes.</option>
<option value="8">8. Both transform raw sound into a
						structured representation, but humans depend entirely on passive hearing mechanisms,
						while ASR can selectively focus on different frequency bands.</option>
<option value="9">9. Lexical and syntactic processing</option>
<option value="10">10. Humans use grammar and context to form meaningful words and sentences based on
						rules.</option>
<option value="11">11. Speech recognition systems employ probabilistic models to predict likely
						word sequences.</option>
<option value="12">12. ASR systems analyze phonemes holistically, considering full words
						before identifying individual phonemes.</option>
<option value="13">13. Semantic and pragmatic understanding</option>
<option value="14">14. Humans interpret intent, tone, and meaning by integrating linguistic input with
						prior knowledge, social cues, and discourse context.</option>
<option value="15">15. Speech recognition systems
						also rely on contextual modeling, but may struggle with implicit meanings, sarcasm, or
						ambiguous phrasing.</option>
<option value="16">16. Both transform raw sound into a structured representation, but
						humans depend entirely on passive hearing mechanisms, while ASR can selectively focus on
						different frequency bands.</option></select><textarea name="best-comment-C-T1-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-C-T1-1"><option value="">Select a segment</option>
<option value="1">1. Acoustic signal processing</option>
<option value="2">2. Humans convert sound waves into neural signals through the cochlea and auditory
						nerve, naturally filtering noise and adapting to variations in speech.</option>
<option value="3">3. ASR systems
						use microphones to capture audio, then apply digital signal processing (e.g., FFT,
						MFCCs) to extract relevant features.</option>
<option value="4">4. Both transform raw sound into a structured
						representation, but humans depend entirely on passive hearing mechanisms, while ASR can
						selectively focus on different frequency bands.</option>
<option value="5">5. Phonetic and phonological analysis</option>
<option value="6">6. Humans segment speech into phonemes based on auditory input, considering
						coarticulation and context.</option>
<option value="7">7. ASR systems analyze phonemes holistically, considering
						full words before identifying individual phonemes.</option>
<option value="8">8. Both transform raw sound into a
						structured representation, but humans depend entirely on passive hearing mechanisms,
						while ASR can selectively focus on different frequency bands.</option>
<option value="9">9. Lexical and syntactic processing</option>
<option value="10">10. Humans use grammar and context to form meaningful words and sentences based on
						rules.</option>
<option value="11">11. Speech recognition systems employ probabilistic models to predict likely
						word sequences.</option>
<option value="12">12. ASR systems analyze phonemes holistically, considering full words
						before identifying individual phonemes.</option>
<option value="13">13. Semantic and pragmatic understanding</option>
<option value="14">14. Humans interpret intent, tone, and meaning by integrating linguistic input with
						prior knowledge, social cues, and discourse context.</option>
<option value="15">15. Speech recognition systems
						also rely on contextual modeling, but may struggle with implicit meanings, sarcasm, or
						ambiguous phrasing.</option>
<option value="16">16. Both transform raw sound into a structured representation, but
						humans depend entirely on passive hearing mechanisms, while ASR can selectively focus on
						different frequency bands.</option></select><textarea name="worst-comment-C-T1-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-C-T1-2"><option value="">Select a segment</option>
<option value="1">1. Acoustic signal processing</option>
<option value="2">2. Humans convert sound waves into neural signals through the cochlea and auditory
						nerve, naturally filtering noise and adapting to variations in speech.</option>
<option value="3">3. ASR systems
						use microphones to capture audio, then apply digital signal processing (e.g., FFT,
						MFCCs) to extract relevant features.</option>
<option value="4">4. Both transform raw sound into a structured
						representation, but humans depend entirely on passive hearing mechanisms, while ASR can
						selectively focus on different frequency bands.</option>
<option value="5">5. Phonetic and phonological analysis</option>
<option value="6">6. Humans segment speech into phonemes based on auditory input, considering
						coarticulation and context.</option>
<option value="7">7. ASR systems analyze phonemes holistically, considering
						full words before identifying individual phonemes.</option>
<option value="8">8. Both transform raw sound into a
						structured representation, but humans depend entirely on passive hearing mechanisms,
						while ASR can selectively focus on different frequency bands.</option>
<option value="9">9. Lexical and syntactic processing</option>
<option value="10">10. Humans use grammar and context to form meaningful words and sentences based on
						rules.</option>
<option value="11">11. Speech recognition systems employ probabilistic models to predict likely
						word sequences.</option>
<option value="12">12. ASR systems analyze phonemes holistically, considering full words
						before identifying individual phonemes.</option>
<option value="13">13. Semantic and pragmatic understanding</option>
<option value="14">14. Humans interpret intent, tone, and meaning by integrating linguistic input with
						prior knowledge, social cues, and discourse context.</option>
<option value="15">15. Speech recognition systems
						also rely on contextual modeling, but may struggle with implicit meanings, sarcasm, or
						ambiguous phrasing.</option>
<option value="16">16. Both transform raw sound into a structured representation, but
						humans depend entirely on passive hearing mechanisms, while ASR can selectively focus on
						different frequency bands.</option></select><textarea name="worst-comment-C-T1-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>C level questions | 2 speech production and speech synthesis</h2>
<h2>Question</h2>
<p> What are four key challenges in developing high-quality Text-to-Speech (TTS) systems,
				and how do they relate to human speech production? Provide an explanation for each
				challenge, including an example where relevant. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Modeling prosody</li>
<li class="segment">
<span class="segment-number">2. </span>Generating speech that sounds natural requires precise control over prosody,
						including intonation, stress, and rhythm.</li>
<li class="segment">
<span class="segment-number">3. </span>Unlike human speakers, TTS systems must
						infer prosodic patterns without access to deep semantic understanding.</li>
<li class="segment">
<span class="segment-number">4. </span>Example: In
						human speech, the phrase "I didn’t say he stole the money" changes meaning depending on
						which word is stressed—a nuance that TTS struggles to capture fully.</li>
<li class="segment">
<span class="segment-number">5. </span>TTS models
						can solve this by strictly following punctuation and word boundaries.</li>
<li class="segment">
<span class="segment-number">6. </span>Modern TTS
						systems incorporate deep learning techniques that analyze broader linguistic context.</li>
<li class="segment">
<span class="segment-number">7. </span>Handling low-resource languages and speaker variability</li>
<li class="segment">
<span class="segment-number">8. </span>Many languages lack large-scale datasets for training TTS models, making it
						difficult to achieve natural-sounding synthesis.</li>
<li class="segment">
<span class="segment-number">9. </span>Example: A TTS system trained
						primarily on English speech may struggle to replicate the tonal qualities of Mandarin
						without significant adaptation.</li>
<li class="segment">
<span class="segment-number">10. </span>Multilingual models and transfer learning
						techniques help improve synthesis quality for low-resource languages.</li>
<li class="segment">
<span class="segment-number">11. </span>Ensuring real-time performance and computational efficiency</li>
<li class="segment">
<span class="segment-number">12. </span>TTS systems must generate speech quickly and efficiently, particularly for
						applications like voice assistants or real-time translation.</li>
<li class="segment">
<span class="segment-number">13. </span>Example: A virtual
						assistant should produce responses with minimal delay while maintaining high audio
						fidelity.</li>
<li class="segment">
<span class="segment-number">14. </span>Optimizing deep learning architectures, such as reducing inference time
						in neural vocoders, helps balance speed and quality.</li>
<li class="segment">
<span class="segment-number">15. </span>Achieving high-quality speech synthesis across voices</li>
<li class="segment">
<span class="segment-number">16. </span>Producing consistent, high-quality speech across multiple voices and speaking
						styles is a challenge.</li>
<li class="segment">
<span class="segment-number">17. </span>Synthetic voices rely on waveform manipulation, and
						struggle with adapting dynamically to speaking style.</li>
<li class="segment">
<span class="segment-number">18. </span>TTS models must generate
						distinct voice characteristics while maintaining clarity and intelligibility.</li>
<li class="segment">
<span class="segment-number">19. </span>Example:
						A single TTS engine may need to synthesize both formal newsreader speech and casual
						conversational tones.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-C-T2-1"><option value="">Select a segment</option>
<option value="1">1. Modeling prosody</option>
<option value="2">2. Generating speech that sounds natural requires precise control over prosody,
						including intonation, stress, and rhythm.</option>
<option value="3">3. Unlike human speakers, TTS systems must
						infer prosodic patterns without access to deep semantic understanding.</option>
<option value="4">4. Example: In
						human speech, the phrase "I didn’t say he stole the money" changes meaning depending on
						which word is stressed—a nuance that TTS struggles to capture fully.</option>
<option value="5">5. TTS models
						can solve this by strictly following punctuation and word boundaries.</option>
<option value="6">6. Modern TTS
						systems incorporate deep learning techniques that analyze broader linguistic context.</option>
<option value="7">7. Handling low-resource languages and speaker variability</option>
<option value="8">8. Many languages lack large-scale datasets for training TTS models, making it
						difficult to achieve natural-sounding synthesis.</option>
<option value="9">9. Example: A TTS system trained
						primarily on English speech may struggle to replicate the tonal qualities of Mandarin
						without significant adaptation.</option>
<option value="10">10. Multilingual models and transfer learning
						techniques help improve synthesis quality for low-resource languages.</option>
<option value="11">11. Ensuring real-time performance and computational efficiency</option>
<option value="12">12. TTS systems must generate speech quickly and efficiently, particularly for
						applications like voice assistants or real-time translation.</option>
<option value="13">13. Example: A virtual
						assistant should produce responses with minimal delay while maintaining high audio
						fidelity.</option>
<option value="14">14. Optimizing deep learning architectures, such as reducing inference time
						in neural vocoders, helps balance speed and quality.</option>
<option value="15">15. Achieving high-quality speech synthesis across voices</option>
<option value="16">16. Producing consistent, high-quality speech across multiple voices and speaking
						styles is a challenge.</option>
<option value="17">17. Synthetic voices rely on waveform manipulation, and
						struggle with adapting dynamically to speaking style.</option>
<option value="18">18. TTS models must generate
						distinct voice characteristics while maintaining clarity and intelligibility.</option>
<option value="19">19. Example:
						A single TTS engine may need to synthesize both formal newsreader speech and casual
						conversational tones.</option></select><textarea name="best-comment-C-T2-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-C-T2-2"><option value="">Select a segment</option>
<option value="1">1. Modeling prosody</option>
<option value="2">2. Generating speech that sounds natural requires precise control over prosody,
						including intonation, stress, and rhythm.</option>
<option value="3">3. Unlike human speakers, TTS systems must
						infer prosodic patterns without access to deep semantic understanding.</option>
<option value="4">4. Example: In
						human speech, the phrase "I didn’t say he stole the money" changes meaning depending on
						which word is stressed—a nuance that TTS struggles to capture fully.</option>
<option value="5">5. TTS models
						can solve this by strictly following punctuation and word boundaries.</option>
<option value="6">6. Modern TTS
						systems incorporate deep learning techniques that analyze broader linguistic context.</option>
<option value="7">7. Handling low-resource languages and speaker variability</option>
<option value="8">8. Many languages lack large-scale datasets for training TTS models, making it
						difficult to achieve natural-sounding synthesis.</option>
<option value="9">9. Example: A TTS system trained
						primarily on English speech may struggle to replicate the tonal qualities of Mandarin
						without significant adaptation.</option>
<option value="10">10. Multilingual models and transfer learning
						techniques help improve synthesis quality for low-resource languages.</option>
<option value="11">11. Ensuring real-time performance and computational efficiency</option>
<option value="12">12. TTS systems must generate speech quickly and efficiently, particularly for
						applications like voice assistants or real-time translation.</option>
<option value="13">13. Example: A virtual
						assistant should produce responses with minimal delay while maintaining high audio
						fidelity.</option>
<option value="14">14. Optimizing deep learning architectures, such as reducing inference time
						in neural vocoders, helps balance speed and quality.</option>
<option value="15">15. Achieving high-quality speech synthesis across voices</option>
<option value="16">16. Producing consistent, high-quality speech across multiple voices and speaking
						styles is a challenge.</option>
<option value="17">17. Synthetic voices rely on waveform manipulation, and
						struggle with adapting dynamically to speaking style.</option>
<option value="18">18. TTS models must generate
						distinct voice characteristics while maintaining clarity and intelligibility.</option>
<option value="19">19. Example:
						A single TTS engine may need to synthesize both formal newsreader speech and casual
						conversational tones.</option></select><textarea name="best-comment-C-T2-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-C-T2-1"><option value="">Select a segment</option>
<option value="1">1. Modeling prosody</option>
<option value="2">2. Generating speech that sounds natural requires precise control over prosody,
						including intonation, stress, and rhythm.</option>
<option value="3">3. Unlike human speakers, TTS systems must
						infer prosodic patterns without access to deep semantic understanding.</option>
<option value="4">4. Example: In
						human speech, the phrase "I didn’t say he stole the money" changes meaning depending on
						which word is stressed—a nuance that TTS struggles to capture fully.</option>
<option value="5">5. TTS models
						can solve this by strictly following punctuation and word boundaries.</option>
<option value="6">6. Modern TTS
						systems incorporate deep learning techniques that analyze broader linguistic context.</option>
<option value="7">7. Handling low-resource languages and speaker variability</option>
<option value="8">8. Many languages lack large-scale datasets for training TTS models, making it
						difficult to achieve natural-sounding synthesis.</option>
<option value="9">9. Example: A TTS system trained
						primarily on English speech may struggle to replicate the tonal qualities of Mandarin
						without significant adaptation.</option>
<option value="10">10. Multilingual models and transfer learning
						techniques help improve synthesis quality for low-resource languages.</option>
<option value="11">11. Ensuring real-time performance and computational efficiency</option>
<option value="12">12. TTS systems must generate speech quickly and efficiently, particularly for
						applications like voice assistants or real-time translation.</option>
<option value="13">13. Example: A virtual
						assistant should produce responses with minimal delay while maintaining high audio
						fidelity.</option>
<option value="14">14. Optimizing deep learning architectures, such as reducing inference time
						in neural vocoders, helps balance speed and quality.</option>
<option value="15">15. Achieving high-quality speech synthesis across voices</option>
<option value="16">16. Producing consistent, high-quality speech across multiple voices and speaking
						styles is a challenge.</option>
<option value="17">17. Synthetic voices rely on waveform manipulation, and
						struggle with adapting dynamically to speaking style.</option>
<option value="18">18. TTS models must generate
						distinct voice characteristics while maintaining clarity and intelligibility.</option>
<option value="19">19. Example:
						A single TTS engine may need to synthesize both formal newsreader speech and casual
						conversational tones.</option></select><textarea name="worst-comment-C-T2-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-C-T2-2"><option value="">Select a segment</option>
<option value="1">1. Modeling prosody</option>
<option value="2">2. Generating speech that sounds natural requires precise control over prosody,
						including intonation, stress, and rhythm.</option>
<option value="3">3. Unlike human speakers, TTS systems must
						infer prosodic patterns without access to deep semantic understanding.</option>
<option value="4">4. Example: In
						human speech, the phrase "I didn’t say he stole the money" changes meaning depending on
						which word is stressed—a nuance that TTS struggles to capture fully.</option>
<option value="5">5. TTS models
						can solve this by strictly following punctuation and word boundaries.</option>
<option value="6">6. Modern TTS
						systems incorporate deep learning techniques that analyze broader linguistic context.</option>
<option value="7">7. Handling low-resource languages and speaker variability</option>
<option value="8">8. Many languages lack large-scale datasets for training TTS models, making it
						difficult to achieve natural-sounding synthesis.</option>
<option value="9">9. Example: A TTS system trained
						primarily on English speech may struggle to replicate the tonal qualities of Mandarin
						without significant adaptation.</option>
<option value="10">10. Multilingual models and transfer learning
						techniques help improve synthesis quality for low-resource languages.</option>
<option value="11">11. Ensuring real-time performance and computational efficiency</option>
<option value="12">12. TTS systems must generate speech quickly and efficiently, particularly for
						applications like voice assistants or real-time translation.</option>
<option value="13">13. Example: A virtual
						assistant should produce responses with minimal delay while maintaining high audio
						fidelity.</option>
<option value="14">14. Optimizing deep learning architectures, such as reducing inference time
						in neural vocoders, helps balance speed and quality.</option>
<option value="15">15. Achieving high-quality speech synthesis across voices</option>
<option value="16">16. Producing consistent, high-quality speech across multiple voices and speaking
						styles is a challenge.</option>
<option value="17">17. Synthetic voices rely on waveform manipulation, and
						struggle with adapting dynamically to speaking style.</option>
<option value="18">18. TTS models must generate
						distinct voice characteristics while maintaining clarity and intelligibility.</option>
<option value="19">19. Example:
						A single TTS engine may need to synthesize both formal newsreader speech and casual
						conversational tones.</option></select><textarea name="worst-comment-C-T2-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>C level questions | 3 dialogue systems</h2>
<h2>Question</h2>
<p> Challenges for dialogue systems in human-like communication: Humans use various
				conversational strategies that are natural for them but challenging for dialogue systems to
				handle effectively. Identify four such strategies and explain why they are difficult for
				machines to process. Provide an example for each. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Understanding and responding to implicit meaning (implicitness &amp; pragmatics)</li>
<li class="segment">
<span class="segment-number">2. </span>Although dialogue systems primarily rely on syntax rather than meaning, they can
						still understand implicit intent accurately.</li>
<li class="segment">
<span class="segment-number">3. </span>Humans infer meaning from context,
						tone, and shared knowledge, while dialogue systems struggle with implicit intent
						recognition.</li>
<li class="segment">
<span class="segment-number">4. </span>Example: A human saying, "It’s cold in here," may be requesting to
						close a window, but a machine might just acknowledge the statement without acting.</li>
<li class="segment">
<span class="segment-number">5. </span>Contextual
						reasoning and pragmatic inference require world knowledge and advanced NLP techniques.</li>
<li class="segment">
<span class="segment-number">6. </span>Handling ambiguity and incomplete sentences</li>
<li class="segment">
<span class="segment-number">7. </span>Humans easily resolve ambiguous statements using context, while machines often
						fail without additional clarification.</li>
<li class="segment">
<span class="segment-number">8. </span>Example: If a user says, "Book a table for
						me," without specifying a time or location, a human would ask for details naturally, but
						a machine might respond incorrectly or fail. Machines rely on explicit instructions and
						often struggle with context-aware follow-ups.</li>
<li class="segment">
<span class="segment-number">9. </span>Because ambiguity is a problem in
						spoken interactions, text-based dialogue systems do not need ambiguity resolution
						mechanisms.</li>
<li class="segment">
<span class="segment-number">10. </span>Adapting to different speaking styles and errors (disfluencies &amp; repairs)</li>
<li class="segment">
<span class="segment-number">11. </span>Humans effortlessly handle speech disfluencies (pauses, corrections,
						hesitations), whereas dialogue systems may misinterpret them.</li>
<li class="segment">
<span class="segment-number">12. </span>Example: A person
						saying, "I need a—uh, I mean, can I book a flight for tomorrow?" is understood by
						humans, but a machine may misinterpret or ignore the correction. NLP models often treat
						speech errors literally instead of understanding self-correction.</li>
<li class="segment">
<span class="segment-number">13. </span>Turn-taking and interruptions in conversation</li>
<li class="segment">
<span class="segment-number">14. </span>Humans manage natural turn-taking and interruptions smoothly, but machines often
						struggle to detect when to speak or stop.</li>
<li class="segment">
<span class="segment-number">15. </span>Example: In human dialogue, people
						overlap slightly in speech but still understand each other.</li>
<li class="segment">
<span class="segment-number">16. </span>A dialogue system
						might wait too long or interrupt at the wrong time.</li>
<li class="segment">
<span class="segment-number">17. </span>Speech processing systems lack real-time adaptation to natural pauses and
						overlapping speech.</li>
<li class="segment">
<span class="segment-number">18. </span>Example: In human dialogue, people overlap slightly in speech but still
						understand each other.</li>
<li class="segment">
<span class="segment-number">19. </span>A dialogue system might wait too long or interrupt at the
						wrong time.</li>
<li class="segment">
<span class="segment-number">20. </span>Speech processing systems lack real-time adaptation to natural pauses
						and overlapping speech.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-C-T3-1"><option value="">Select a segment</option>
<option value="1">1. Understanding and responding to implicit meaning (implicitness &amp; pragmatics)</option>
<option value="2">2. Although dialogue systems primarily rely on syntax rather than meaning, they can
						still understand implicit intent accurately.</option>
<option value="3">3. Humans infer meaning from context,
						tone, and shared knowledge, while dialogue systems struggle with implicit intent
						recognition.</option>
<option value="4">4. Example: A human saying, "It’s cold in here," may be requesting to
						close a window, but a machine might just acknowledge the statement without acting.</option>
<option value="5">5. Contextual
						reasoning and pragmatic inference require world knowledge and advanced NLP techniques.</option>
<option value="6">6. Handling ambiguity and incomplete sentences</option>
<option value="7">7. Humans easily resolve ambiguous statements using context, while machines often
						fail without additional clarification.</option>
<option value="8">8. Example: If a user says, "Book a table for
						me," without specifying a time or location, a human would ask for details naturally, but
						a machine might respond incorrectly or fail. Machines rely on explicit instructions and
						often struggle with context-aware follow-ups.</option>
<option value="9">9. Because ambiguity is a problem in
						spoken interactions, text-based dialogue systems do not need ambiguity resolution
						mechanisms.</option>
<option value="10">10. Adapting to different speaking styles and errors (disfluencies &amp; repairs)</option>
<option value="11">11. Humans effortlessly handle speech disfluencies (pauses, corrections,
						hesitations), whereas dialogue systems may misinterpret them.</option>
<option value="12">12. Example: A person
						saying, "I need a—uh, I mean, can I book a flight for tomorrow?" is understood by
						humans, but a machine may misinterpret or ignore the correction. NLP models often treat
						speech errors literally instead of understanding self-correction.</option>
<option value="13">13. Turn-taking and interruptions in conversation</option>
<option value="14">14. Humans manage natural turn-taking and interruptions smoothly, but machines often
						struggle to detect when to speak or stop.</option>
<option value="15">15. Example: In human dialogue, people
						overlap slightly in speech but still understand each other.</option>
<option value="16">16. A dialogue system
						might wait too long or interrupt at the wrong time.</option>
<option value="17">17. Speech processing systems lack real-time adaptation to natural pauses and
						overlapping speech.</option>
<option value="18">18. Example: In human dialogue, people overlap slightly in speech but still
						understand each other.</option>
<option value="19">19. A dialogue system might wait too long or interrupt at the
						wrong time.</option>
<option value="20">20. Speech processing systems lack real-time adaptation to natural pauses
						and overlapping speech.</option></select><textarea name="best-comment-C-T3-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-C-T3-2"><option value="">Select a segment</option>
<option value="1">1. Understanding and responding to implicit meaning (implicitness &amp; pragmatics)</option>
<option value="2">2. Although dialogue systems primarily rely on syntax rather than meaning, they can
						still understand implicit intent accurately.</option>
<option value="3">3. Humans infer meaning from context,
						tone, and shared knowledge, while dialogue systems struggle with implicit intent
						recognition.</option>
<option value="4">4. Example: A human saying, "It’s cold in here," may be requesting to
						close a window, but a machine might just acknowledge the statement without acting.</option>
<option value="5">5. Contextual
						reasoning and pragmatic inference require world knowledge and advanced NLP techniques.</option>
<option value="6">6. Handling ambiguity and incomplete sentences</option>
<option value="7">7. Humans easily resolve ambiguous statements using context, while machines often
						fail without additional clarification.</option>
<option value="8">8. Example: If a user says, "Book a table for
						me," without specifying a time or location, a human would ask for details naturally, but
						a machine might respond incorrectly or fail. Machines rely on explicit instructions and
						often struggle with context-aware follow-ups.</option>
<option value="9">9. Because ambiguity is a problem in
						spoken interactions, text-based dialogue systems do not need ambiguity resolution
						mechanisms.</option>
<option value="10">10. Adapting to different speaking styles and errors (disfluencies &amp; repairs)</option>
<option value="11">11. Humans effortlessly handle speech disfluencies (pauses, corrections,
						hesitations), whereas dialogue systems may misinterpret them.</option>
<option value="12">12. Example: A person
						saying, "I need a—uh, I mean, can I book a flight for tomorrow?" is understood by
						humans, but a machine may misinterpret or ignore the correction. NLP models often treat
						speech errors literally instead of understanding self-correction.</option>
<option value="13">13. Turn-taking and interruptions in conversation</option>
<option value="14">14. Humans manage natural turn-taking and interruptions smoothly, but machines often
						struggle to detect when to speak or stop.</option>
<option value="15">15. Example: In human dialogue, people
						overlap slightly in speech but still understand each other.</option>
<option value="16">16. A dialogue system
						might wait too long or interrupt at the wrong time.</option>
<option value="17">17. Speech processing systems lack real-time adaptation to natural pauses and
						overlapping speech.</option>
<option value="18">18. Example: In human dialogue, people overlap slightly in speech but still
						understand each other.</option>
<option value="19">19. A dialogue system might wait too long or interrupt at the
						wrong time.</option>
<option value="20">20. Speech processing systems lack real-time adaptation to natural pauses
						and overlapping speech.</option></select><textarea name="best-comment-C-T3-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-C-T3-1"><option value="">Select a segment</option>
<option value="1">1. Understanding and responding to implicit meaning (implicitness &amp; pragmatics)</option>
<option value="2">2. Although dialogue systems primarily rely on syntax rather than meaning, they can
						still understand implicit intent accurately.</option>
<option value="3">3. Humans infer meaning from context,
						tone, and shared knowledge, while dialogue systems struggle with implicit intent
						recognition.</option>
<option value="4">4. Example: A human saying, "It’s cold in here," may be requesting to
						close a window, but a machine might just acknowledge the statement without acting.</option>
<option value="5">5. Contextual
						reasoning and pragmatic inference require world knowledge and advanced NLP techniques.</option>
<option value="6">6. Handling ambiguity and incomplete sentences</option>
<option value="7">7. Humans easily resolve ambiguous statements using context, while machines often
						fail without additional clarification.</option>
<option value="8">8. Example: If a user says, "Book a table for
						me," without specifying a time or location, a human would ask for details naturally, but
						a machine might respond incorrectly or fail. Machines rely on explicit instructions and
						often struggle with context-aware follow-ups.</option>
<option value="9">9. Because ambiguity is a problem in
						spoken interactions, text-based dialogue systems do not need ambiguity resolution
						mechanisms.</option>
<option value="10">10. Adapting to different speaking styles and errors (disfluencies &amp; repairs)</option>
<option value="11">11. Humans effortlessly handle speech disfluencies (pauses, corrections,
						hesitations), whereas dialogue systems may misinterpret them.</option>
<option value="12">12. Example: A person
						saying, "I need a—uh, I mean, can I book a flight for tomorrow?" is understood by
						humans, but a machine may misinterpret or ignore the correction. NLP models often treat
						speech errors literally instead of understanding self-correction.</option>
<option value="13">13. Turn-taking and interruptions in conversation</option>
<option value="14">14. Humans manage natural turn-taking and interruptions smoothly, but machines often
						struggle to detect when to speak or stop.</option>
<option value="15">15. Example: In human dialogue, people
						overlap slightly in speech but still understand each other.</option>
<option value="16">16. A dialogue system
						might wait too long or interrupt at the wrong time.</option>
<option value="17">17. Speech processing systems lack real-time adaptation to natural pauses and
						overlapping speech.</option>
<option value="18">18. Example: In human dialogue, people overlap slightly in speech but still
						understand each other.</option>
<option value="19">19. A dialogue system might wait too long or interrupt at the
						wrong time.</option>
<option value="20">20. Speech processing systems lack real-time adaptation to natural pauses
						and overlapping speech.</option></select><textarea name="worst-comment-C-T3-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-C-T3-2"><option value="">Select a segment</option>
<option value="1">1. Understanding and responding to implicit meaning (implicitness &amp; pragmatics)</option>
<option value="2">2. Although dialogue systems primarily rely on syntax rather than meaning, they can
						still understand implicit intent accurately.</option>
<option value="3">3. Humans infer meaning from context,
						tone, and shared knowledge, while dialogue systems struggle with implicit intent
						recognition.</option>
<option value="4">4. Example: A human saying, "It’s cold in here," may be requesting to
						close a window, but a machine might just acknowledge the statement without acting.</option>
<option value="5">5. Contextual
						reasoning and pragmatic inference require world knowledge and advanced NLP techniques.</option>
<option value="6">6. Handling ambiguity and incomplete sentences</option>
<option value="7">7. Humans easily resolve ambiguous statements using context, while machines often
						fail without additional clarification.</option>
<option value="8">8. Example: If a user says, "Book a table for
						me," without specifying a time or location, a human would ask for details naturally, but
						a machine might respond incorrectly or fail. Machines rely on explicit instructions and
						often struggle with context-aware follow-ups.</option>
<option value="9">9. Because ambiguity is a problem in
						spoken interactions, text-based dialogue systems do not need ambiguity resolution
						mechanisms.</option>
<option value="10">10. Adapting to different speaking styles and errors (disfluencies &amp; repairs)</option>
<option value="11">11. Humans effortlessly handle speech disfluencies (pauses, corrections,
						hesitations), whereas dialogue systems may misinterpret them.</option>
<option value="12">12. Example: A person
						saying, "I need a—uh, I mean, can I book a flight for tomorrow?" is understood by
						humans, but a machine may misinterpret or ignore the correction. NLP models often treat
						speech errors literally instead of understanding self-correction.</option>
<option value="13">13. Turn-taking and interruptions in conversation</option>
<option value="14">14. Humans manage natural turn-taking and interruptions smoothly, but machines often
						struggle to detect when to speak or stop.</option>
<option value="15">15. Example: In human dialogue, people
						overlap slightly in speech but still understand each other.</option>
<option value="16">16. A dialogue system
						might wait too long or interrupt at the wrong time.</option>
<option value="17">17. Speech processing systems lack real-time adaptation to natural pauses and
						overlapping speech.</option>
<option value="18">18. Example: In human dialogue, people overlap slightly in speech but still
						understand each other.</option>
<option value="19">19. A dialogue system might wait too long or interrupt at the
						wrong time.</option>
<option value="20">20. Speech processing systems lack real-time adaptation to natural pauses
						and overlapping speech.</option></select><textarea name="worst-comment-C-T3-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>C level questions | 4 data collection</h2>
<h2>Question</h2>
<p> Designing a Data Collection Procedure for Virtual Meetings: You are tasked with
				designing a data collection procedure to study communication efficiency in virtual video
				meetings. Outline four key steps in your data collection process. For each step, explain
				what considerations must be taken into account and provide an example of how it impacts the
				quality of the collected data. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Defining research objectives and metrics</li>
<li class="segment">
<span class="segment-number">2. </span>Before collecting data, it is crucial to define what aspects of virtual meetings
						will be studied (e.g., speaking time, interruptions, engagement).</li>
<li class="segment">
<span class="segment-number">3. </span>Example: If
						studying communication efficiency, relevant metrics might include turn-taking frequency,
						latency in responses, and speaking balance.</li>
<li class="segment">
<span class="segment-number">4. </span>Consideration: Clearly defined
						objectives ensure that the collected data is meaningful and aligned with the study’s
						goals, but broad objectives should always be preferred over specific ones to allow
						flexible analysis later.</li>
<li class="segment">
<span class="segment-number">5. </span>Selecting participants and meeting scenarios</li>
<li class="segment">
<span class="segment-number">6. </span>Data should be collected from a diverse set of participants to capture a range of
						communication styles and technical conditions.</li>
<li class="segment">
<span class="segment-number">7. </span>Example: Including both experienced
						remote workers and those new to virtual meetings provides more generalizable results.</li>
<li class="segment">
<span class="segment-number">8. </span>Consideration:
						Although participant demographics have minimal impact on communication patterns in
						virtual meetings, ensuring a balanced participant pool prevents bias in the findings.</li>
<li class="segment">
<span class="segment-number">9. </span>Choosing data collection methods and tools</li>
<li class="segment">
<span class="segment-number">10. </span>Various methods (e.g., recording video/audio, transcribing speech, analyzing chat
						logs) can be used to gather data.</li>
<li class="segment">
<span class="segment-number">11. </span>Example: Using automatic speech recognition
						(ASR) for transcription enables quantitative analysis of dialogue structure and
						efficiency.</li>
<li class="segment">
<span class="segment-number">12. </span>Consideration: The chosen tools must be reliable, non-intrusive, and
						capable of capturing relevant aspects of communication.</li>
<li class="segment">
<span class="segment-number">13. </span>Ethical considerations and data privacy</li>
<li class="segment">
<span class="segment-number">14. </span>Since video meetings involve personal interactions, obtaining informed consent
						and ensuring data security is essential.</li>
<li class="segment">
<span class="segment-number">15. </span>Example: Participants should be informed
						about what data is being recorded (e.g., speech, video, chat logs) and how it will be
						used.</li>
<li class="segment">
<span class="segment-number">16. </span>Consideration: Adhering to GDPR or other privacy regulations is crucial when
						handling sensitive data.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-C-T4-1"><option value="">Select a segment</option>
<option value="1">1. Defining research objectives and metrics</option>
<option value="2">2. Before collecting data, it is crucial to define what aspects of virtual meetings
						will be studied (e.g., speaking time, interruptions, engagement).</option>
<option value="3">3. Example: If
						studying communication efficiency, relevant metrics might include turn-taking frequency,
						latency in responses, and speaking balance.</option>
<option value="4">4. Consideration: Clearly defined
						objectives ensure that the collected data is meaningful and aligned with the study’s
						goals, but broad objectives should always be preferred over specific ones to allow
						flexible analysis later.</option>
<option value="5">5. Selecting participants and meeting scenarios</option>
<option value="6">6. Data should be collected from a diverse set of participants to capture a range of
						communication styles and technical conditions.</option>
<option value="7">7. Example: Including both experienced
						remote workers and those new to virtual meetings provides more generalizable results.</option>
<option value="8">8. Consideration:
						Although participant demographics have minimal impact on communication patterns in
						virtual meetings, ensuring a balanced participant pool prevents bias in the findings.</option>
<option value="9">9. Choosing data collection methods and tools</option>
<option value="10">10. Various methods (e.g., recording video/audio, transcribing speech, analyzing chat
						logs) can be used to gather data.</option>
<option value="11">11. Example: Using automatic speech recognition
						(ASR) for transcription enables quantitative analysis of dialogue structure and
						efficiency.</option>
<option value="12">12. Consideration: The chosen tools must be reliable, non-intrusive, and
						capable of capturing relevant aspects of communication.</option>
<option value="13">13. Ethical considerations and data privacy</option>
<option value="14">14. Since video meetings involve personal interactions, obtaining informed consent
						and ensuring data security is essential.</option>
<option value="15">15. Example: Participants should be informed
						about what data is being recorded (e.g., speech, video, chat logs) and how it will be
						used.</option>
<option value="16">16. Consideration: Adhering to GDPR or other privacy regulations is crucial when
						handling sensitive data.</option></select><textarea name="best-comment-C-T4-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-C-T4-2"><option value="">Select a segment</option>
<option value="1">1. Defining research objectives and metrics</option>
<option value="2">2. Before collecting data, it is crucial to define what aspects of virtual meetings
						will be studied (e.g., speaking time, interruptions, engagement).</option>
<option value="3">3. Example: If
						studying communication efficiency, relevant metrics might include turn-taking frequency,
						latency in responses, and speaking balance.</option>
<option value="4">4. Consideration: Clearly defined
						objectives ensure that the collected data is meaningful and aligned with the study’s
						goals, but broad objectives should always be preferred over specific ones to allow
						flexible analysis later.</option>
<option value="5">5. Selecting participants and meeting scenarios</option>
<option value="6">6. Data should be collected from a diverse set of participants to capture a range of
						communication styles and technical conditions.</option>
<option value="7">7. Example: Including both experienced
						remote workers and those new to virtual meetings provides more generalizable results.</option>
<option value="8">8. Consideration:
						Although participant demographics have minimal impact on communication patterns in
						virtual meetings, ensuring a balanced participant pool prevents bias in the findings.</option>
<option value="9">9. Choosing data collection methods and tools</option>
<option value="10">10. Various methods (e.g., recording video/audio, transcribing speech, analyzing chat
						logs) can be used to gather data.</option>
<option value="11">11. Example: Using automatic speech recognition
						(ASR) for transcription enables quantitative analysis of dialogue structure and
						efficiency.</option>
<option value="12">12. Consideration: The chosen tools must be reliable, non-intrusive, and
						capable of capturing relevant aspects of communication.</option>
<option value="13">13. Ethical considerations and data privacy</option>
<option value="14">14. Since video meetings involve personal interactions, obtaining informed consent
						and ensuring data security is essential.</option>
<option value="15">15. Example: Participants should be informed
						about what data is being recorded (e.g., speech, video, chat logs) and how it will be
						used.</option>
<option value="16">16. Consideration: Adhering to GDPR or other privacy regulations is crucial when
						handling sensitive data.</option></select><textarea name="best-comment-C-T4-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-C-T4-1"><option value="">Select a segment</option>
<option value="1">1. Defining research objectives and metrics</option>
<option value="2">2. Before collecting data, it is crucial to define what aspects of virtual meetings
						will be studied (e.g., speaking time, interruptions, engagement).</option>
<option value="3">3. Example: If
						studying communication efficiency, relevant metrics might include turn-taking frequency,
						latency in responses, and speaking balance.</option>
<option value="4">4. Consideration: Clearly defined
						objectives ensure that the collected data is meaningful and aligned with the study’s
						goals, but broad objectives should always be preferred over specific ones to allow
						flexible analysis later.</option>
<option value="5">5. Selecting participants and meeting scenarios</option>
<option value="6">6. Data should be collected from a diverse set of participants to capture a range of
						communication styles and technical conditions.</option>
<option value="7">7. Example: Including both experienced
						remote workers and those new to virtual meetings provides more generalizable results.</option>
<option value="8">8. Consideration:
						Although participant demographics have minimal impact on communication patterns in
						virtual meetings, ensuring a balanced participant pool prevents bias in the findings.</option>
<option value="9">9. Choosing data collection methods and tools</option>
<option value="10">10. Various methods (e.g., recording video/audio, transcribing speech, analyzing chat
						logs) can be used to gather data.</option>
<option value="11">11. Example: Using automatic speech recognition
						(ASR) for transcription enables quantitative analysis of dialogue structure and
						efficiency.</option>
<option value="12">12. Consideration: The chosen tools must be reliable, non-intrusive, and
						capable of capturing relevant aspects of communication.</option>
<option value="13">13. Ethical considerations and data privacy</option>
<option value="14">14. Since video meetings involve personal interactions, obtaining informed consent
						and ensuring data security is essential.</option>
<option value="15">15. Example: Participants should be informed
						about what data is being recorded (e.g., speech, video, chat logs) and how it will be
						used.</option>
<option value="16">16. Consideration: Adhering to GDPR or other privacy regulations is crucial when
						handling sensitive data.</option></select><textarea name="worst-comment-C-T4-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-C-T4-2"><option value="">Select a segment</option>
<option value="1">1. Defining research objectives and metrics</option>
<option value="2">2. Before collecting data, it is crucial to define what aspects of virtual meetings
						will be studied (e.g., speaking time, interruptions, engagement).</option>
<option value="3">3. Example: If
						studying communication efficiency, relevant metrics might include turn-taking frequency,
						latency in responses, and speaking balance.</option>
<option value="4">4. Consideration: Clearly defined
						objectives ensure that the collected data is meaningful and aligned with the study’s
						goals, but broad objectives should always be preferred over specific ones to allow
						flexible analysis later.</option>
<option value="5">5. Selecting participants and meeting scenarios</option>
<option value="6">6. Data should be collected from a diverse set of participants to capture a range of
						communication styles and technical conditions.</option>
<option value="7">7. Example: Including both experienced
						remote workers and those new to virtual meetings provides more generalizable results.</option>
<option value="8">8. Consideration:
						Although participant demographics have minimal impact on communication patterns in
						virtual meetings, ensuring a balanced participant pool prevents bias in the findings.</option>
<option value="9">9. Choosing data collection methods and tools</option>
<option value="10">10. Various methods (e.g., recording video/audio, transcribing speech, analyzing chat
						logs) can be used to gather data.</option>
<option value="11">11. Example: Using automatic speech recognition
						(ASR) for transcription enables quantitative analysis of dialogue structure and
						efficiency.</option>
<option value="12">12. Consideration: The chosen tools must be reliable, non-intrusive, and
						capable of capturing relevant aspects of communication.</option>
<option value="13">13. Ethical considerations and data privacy</option>
<option value="14">14. Since video meetings involve personal interactions, obtaining informed consent
						and ensuring data security is essential.</option>
<option value="15">15. Example: Participants should be informed
						about what data is being recorded (e.g., speech, video, chat logs) and how it will be
						used.</option>
<option value="16">16. Consideration: Adhering to GDPR or other privacy regulations is crucial when
						handling sensitive data.</option></select><textarea name="worst-comment-C-T4-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>C level questions | 5 evaluation</h2>
<h2>Question</h2>
<p> Corpora are collected for different purposes, influencing key design choices. Select
				three important factors for each of the following corpora, explaining why they are crucial: A corpus for training acoustic models for recognizing street names in carsA
				corpus for broad vocabulary synthesis of academic literatureA corpus for studying
				semantic concepts in museum dialogue systemsA corpus for studying breath, posture,
				and speech in turn-taking</p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>A corpus for training acoustic models for in-car speech recognition</li>
<li class="segment">
<span class="segment-number">2. </span>Audio quality: Essential for accurately capturing speech sounds in noisy
						environments.</li>
<li class="segment">
<span class="segment-number">3. </span>Control over the environment: Ensures realistic in-car conditions,
						such as handling road noise.</li>
<li class="segment">
<span class="segment-number">4. </span>Large single-speaker datasets: Provide detailed
						acoustic information for robust models.</li>
<li class="segment">
<span class="segment-number">5. </span>A corpus for broad vocabulary synthesis of academic literature</li>
<li class="segment">
<span class="segment-number">6. </span>Single speaking style: Read speech only to match the requirements.</li>
<li class="segment">
<span class="segment-number">7. </span>Control
						over linguistic background: Consistent pronunciation and phonetic coverage improve
						synthesis quality.</li>
<li class="segment">
<span class="segment-number">8. </span>Control over subjects' pre-recording behavior: Helps maintain
						uniform voice quality.</li>
<li class="segment">
<span class="segment-number">9. </span>A corpus for studying semantic concepts in museum dialogue systems</li>
<li class="segment">
<span class="segment-number">10. </span>An ecologically valid environment: Captures interactions in a real museum
						setting.</li>
<li class="segment">
<span class="segment-number">11. </span>An ecologically valid task: Ensures that dialogues reflect real visitor
						interactions.</li>
<li class="segment">
<span class="segment-number">12. </span>Control over linguistic background: Helps maintain consistency in
						understanding key semantic structures.</li>
<li class="segment">
<span class="segment-number">13. </span>A corpus for studying breath, posture, and speech in turn-taking</li>
<li class="segment">
<span class="segment-number">14. </span>Video quality: Essential for capturing non-verbal cues.</li>
<li class="segment">
<span class="segment-number">15. </span>Synchronization of
						different recordings: Aligns audio and visual data for accurate analysis.</li>
<li class="segment">
<span class="segment-number">16. </span>Mobility
						of subjects: Allows natural interaction patterns to emerge.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-C-T5-1"><option value="">Select a segment</option>
<option value="1">1. A corpus for training acoustic models for in-car speech recognition</option>
<option value="2">2. Audio quality: Essential for accurately capturing speech sounds in noisy
						environments.</option>
<option value="3">3. Control over the environment: Ensures realistic in-car conditions,
						such as handling road noise.</option>
<option value="4">4. Large single-speaker datasets: Provide detailed
						acoustic information for robust models.</option>
<option value="5">5. A corpus for broad vocabulary synthesis of academic literature</option>
<option value="6">6. Single speaking style: Read speech only to match the requirements.</option>
<option value="7">7. Control
						over linguistic background: Consistent pronunciation and phonetic coverage improve
						synthesis quality.</option>
<option value="8">8. Control over subjects' pre-recording behavior: Helps maintain
						uniform voice quality.</option>
<option value="9">9. A corpus for studying semantic concepts in museum dialogue systems</option>
<option value="10">10. An ecologically valid environment: Captures interactions in a real museum
						setting.</option>
<option value="11">11. An ecologically valid task: Ensures that dialogues reflect real visitor
						interactions.</option>
<option value="12">12. Control over linguistic background: Helps maintain consistency in
						understanding key semantic structures.</option>
<option value="13">13. A corpus for studying breath, posture, and speech in turn-taking</option>
<option value="14">14. Video quality: Essential for capturing non-verbal cues.</option>
<option value="15">15. Synchronization of
						different recordings: Aligns audio and visual data for accurate analysis.</option>
<option value="16">16. Mobility
						of subjects: Allows natural interaction patterns to emerge.</option></select><textarea name="best-comment-C-T5-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-C-T5-2"><option value="">Select a segment</option>
<option value="1">1. A corpus for training acoustic models for in-car speech recognition</option>
<option value="2">2. Audio quality: Essential for accurately capturing speech sounds in noisy
						environments.</option>
<option value="3">3. Control over the environment: Ensures realistic in-car conditions,
						such as handling road noise.</option>
<option value="4">4. Large single-speaker datasets: Provide detailed
						acoustic information for robust models.</option>
<option value="5">5. A corpus for broad vocabulary synthesis of academic literature</option>
<option value="6">6. Single speaking style: Read speech only to match the requirements.</option>
<option value="7">7. Control
						over linguistic background: Consistent pronunciation and phonetic coverage improve
						synthesis quality.</option>
<option value="8">8. Control over subjects' pre-recording behavior: Helps maintain
						uniform voice quality.</option>
<option value="9">9. A corpus for studying semantic concepts in museum dialogue systems</option>
<option value="10">10. An ecologically valid environment: Captures interactions in a real museum
						setting.</option>
<option value="11">11. An ecologically valid task: Ensures that dialogues reflect real visitor
						interactions.</option>
<option value="12">12. Control over linguistic background: Helps maintain consistency in
						understanding key semantic structures.</option>
<option value="13">13. A corpus for studying breath, posture, and speech in turn-taking</option>
<option value="14">14. Video quality: Essential for capturing non-verbal cues.</option>
<option value="15">15. Synchronization of
						different recordings: Aligns audio and visual data for accurate analysis.</option>
<option value="16">16. Mobility
						of subjects: Allows natural interaction patterns to emerge.</option></select><textarea name="best-comment-C-T5-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-C-T5-1"><option value="">Select a segment</option>
<option value="1">1. A corpus for training acoustic models for in-car speech recognition</option>
<option value="2">2. Audio quality: Essential for accurately capturing speech sounds in noisy
						environments.</option>
<option value="3">3. Control over the environment: Ensures realistic in-car conditions,
						such as handling road noise.</option>
<option value="4">4. Large single-speaker datasets: Provide detailed
						acoustic information for robust models.</option>
<option value="5">5. A corpus for broad vocabulary synthesis of academic literature</option>
<option value="6">6. Single speaking style: Read speech only to match the requirements.</option>
<option value="7">7. Control
						over linguistic background: Consistent pronunciation and phonetic coverage improve
						synthesis quality.</option>
<option value="8">8. Control over subjects' pre-recording behavior: Helps maintain
						uniform voice quality.</option>
<option value="9">9. A corpus for studying semantic concepts in museum dialogue systems</option>
<option value="10">10. An ecologically valid environment: Captures interactions in a real museum
						setting.</option>
<option value="11">11. An ecologically valid task: Ensures that dialogues reflect real visitor
						interactions.</option>
<option value="12">12. Control over linguistic background: Helps maintain consistency in
						understanding key semantic structures.</option>
<option value="13">13. A corpus for studying breath, posture, and speech in turn-taking</option>
<option value="14">14. Video quality: Essential for capturing non-verbal cues.</option>
<option value="15">15. Synchronization of
						different recordings: Aligns audio and visual data for accurate analysis.</option>
<option value="16">16. Mobility
						of subjects: Allows natural interaction patterns to emerge.</option></select><textarea name="worst-comment-C-T5-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-C-T5-2"><option value="">Select a segment</option>
<option value="1">1. A corpus for training acoustic models for in-car speech recognition</option>
<option value="2">2. Audio quality: Essential for accurately capturing speech sounds in noisy
						environments.</option>
<option value="3">3. Control over the environment: Ensures realistic in-car conditions,
						such as handling road noise.</option>
<option value="4">4. Large single-speaker datasets: Provide detailed
						acoustic information for robust models.</option>
<option value="5">5. A corpus for broad vocabulary synthesis of academic literature</option>
<option value="6">6. Single speaking style: Read speech only to match the requirements.</option>
<option value="7">7. Control
						over linguistic background: Consistent pronunciation and phonetic coverage improve
						synthesis quality.</option>
<option value="8">8. Control over subjects' pre-recording behavior: Helps maintain
						uniform voice quality.</option>
<option value="9">9. A corpus for studying semantic concepts in museum dialogue systems</option>
<option value="10">10. An ecologically valid environment: Captures interactions in a real museum
						setting.</option>
<option value="11">11. An ecologically valid task: Ensures that dialogues reflect real visitor
						interactions.</option>
<option value="12">12. Control over linguistic background: Helps maintain consistency in
						understanding key semantic structures.</option>
<option value="13">13. A corpus for studying breath, posture, and speech in turn-taking</option>
<option value="14">14. Video quality: Essential for capturing non-verbal cues.</option>
<option value="15">15. Synchronization of
						different recordings: Aligns audio and visual data for accurate analysis.</option>
<option value="16">16. Mobility
						of subjects: Allows natural interaction patterns to emerge.</option></select><textarea name="worst-comment-C-T5-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>A level questions | 1 perception and speech recognition</h2>
<h2>Question</h2>
<p> Describe two different ways in which something that is not directly a part of an
				utterance (i.e. something that isn’t a phoneme or a word) can affect our perception of the
				utterance, and how this effect takes place. You may suggest anything, including external
				sounds, visual events, events that took place long before, just before, or after the speech
				sound in question was spoken... For each of your two suggestions, discuss briefly how this
				may affect automatic speech recognition, as compared to how it affects human perception. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Visual context and the McGurk Effect</li>
<li class="segment">
<span class="segment-number">2. </span>Visual information, such as lip movements, can influence how we perceive speech.</li>
<li class="segment">
<span class="segment-number">3. </span>The
						McGurk effect demonstrates that when conflicting audio and visual cues are presented,
						listeners may perceive a different sound than what is actually spoken.</li>
<li class="segment">
<span class="segment-number">4. </span>Example:
						Hearing "ba" while seeing lips articulate "ga" can make someone perceive "da" instead.</li>
<li class="segment">
<span class="segment-number">5. </span>In
						humans, this is due to multimodal integration of sensory input.</li>
<li class="segment">
<span class="segment-number">6. </span>ASR systems,
						however, do not integrate visual cues and thus do not experience this effect.</li>
<li class="segment">
<span class="segment-number">7. </span>Contextual priming from prior speech</li>
<li class="segment">
<span class="segment-number">8. </span>Our perception of an utterance is shaped by what we have recently heard.</li>
<li class="segment">
<span class="segment-number">9. </span>If
						a prior sentence biases interpretation, ambiguous words or phonemes may be perceived
						differently.</li>
<li class="segment">
<span class="segment-number">10. </span>Example: If a person previously hears "He bought a..." followed by a
						noisy or unclear word, they may expect "car" in a vehicular context but "carton" in a
						grocery context.</li>
<li class="segment">
<span class="segment-number">11. </span>ASR systems resolve this by relying entirely on acoustic input,
						making them virtually immune to priming effects.</li>
<li class="segment">
<span class="segment-number">12. </span>Background noise and masking effects</li>
<li class="segment">
<span class="segment-number">13. </span>Non-speech sounds can interfere with speech perception by masking parts of the
						speech signal.</li>
<li class="segment">
<span class="segment-number">14. </span>Loud background noise, such as traffic or music, can make phonemes
						difficult to distinguish.</li>
<li class="segment">
<span class="segment-number">15. </span>Example: A sudden loud noise coinciding with a word
						onset can cause misperception.</li>
<li class="segment">
<span class="segment-number">16. </span>Humans compensate using cognitive processing and
						redundancy, while ASR systems apply noise reduction algorithms, which can fully
						compensate for background noise given proper filtering.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-A-T1-1"><option value="">Select a segment</option>
<option value="1">1. Visual context and the McGurk Effect</option>
<option value="2">2. Visual information, such as lip movements, can influence how we perceive speech.</option>
<option value="3">3. The
						McGurk effect demonstrates that when conflicting audio and visual cues are presented,
						listeners may perceive a different sound than what is actually spoken.</option>
<option value="4">4. Example:
						Hearing "ba" while seeing lips articulate "ga" can make someone perceive "da" instead.</option>
<option value="5">5. In
						humans, this is due to multimodal integration of sensory input.</option>
<option value="6">6. ASR systems,
						however, do not integrate visual cues and thus do not experience this effect.</option>
<option value="7">7. Contextual priming from prior speech</option>
<option value="8">8. Our perception of an utterance is shaped by what we have recently heard.</option>
<option value="9">9. If
						a prior sentence biases interpretation, ambiguous words or phonemes may be perceived
						differently.</option>
<option value="10">10. Example: If a person previously hears "He bought a..." followed by a
						noisy or unclear word, they may expect "car" in a vehicular context but "carton" in a
						grocery context.</option>
<option value="11">11. ASR systems resolve this by relying entirely on acoustic input,
						making them virtually immune to priming effects.</option>
<option value="12">12. Background noise and masking effects</option>
<option value="13">13. Non-speech sounds can interfere with speech perception by masking parts of the
						speech signal.</option>
<option value="14">14. Loud background noise, such as traffic or music, can make phonemes
						difficult to distinguish.</option>
<option value="15">15. Example: A sudden loud noise coinciding with a word
						onset can cause misperception.</option>
<option value="16">16. Humans compensate using cognitive processing and
						redundancy, while ASR systems apply noise reduction algorithms, which can fully
						compensate for background noise given proper filtering.</option></select><textarea name="best-comment-A-T1-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-A-T1-2"><option value="">Select a segment</option>
<option value="1">1. Visual context and the McGurk Effect</option>
<option value="2">2. Visual information, such as lip movements, can influence how we perceive speech.</option>
<option value="3">3. The
						McGurk effect demonstrates that when conflicting audio and visual cues are presented,
						listeners may perceive a different sound than what is actually spoken.</option>
<option value="4">4. Example:
						Hearing "ba" while seeing lips articulate "ga" can make someone perceive "da" instead.</option>
<option value="5">5. In
						humans, this is due to multimodal integration of sensory input.</option>
<option value="6">6. ASR systems,
						however, do not integrate visual cues and thus do not experience this effect.</option>
<option value="7">7. Contextual priming from prior speech</option>
<option value="8">8. Our perception of an utterance is shaped by what we have recently heard.</option>
<option value="9">9. If
						a prior sentence biases interpretation, ambiguous words or phonemes may be perceived
						differently.</option>
<option value="10">10. Example: If a person previously hears "He bought a..." followed by a
						noisy or unclear word, they may expect "car" in a vehicular context but "carton" in a
						grocery context.</option>
<option value="11">11. ASR systems resolve this by relying entirely on acoustic input,
						making them virtually immune to priming effects.</option>
<option value="12">12. Background noise and masking effects</option>
<option value="13">13. Non-speech sounds can interfere with speech perception by masking parts of the
						speech signal.</option>
<option value="14">14. Loud background noise, such as traffic or music, can make phonemes
						difficult to distinguish.</option>
<option value="15">15. Example: A sudden loud noise coinciding with a word
						onset can cause misperception.</option>
<option value="16">16. Humans compensate using cognitive processing and
						redundancy, while ASR systems apply noise reduction algorithms, which can fully
						compensate for background noise given proper filtering.</option></select><textarea name="best-comment-A-T1-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-A-T1-1"><option value="">Select a segment</option>
<option value="1">1. Visual context and the McGurk Effect</option>
<option value="2">2. Visual information, such as lip movements, can influence how we perceive speech.</option>
<option value="3">3. The
						McGurk effect demonstrates that when conflicting audio and visual cues are presented,
						listeners may perceive a different sound than what is actually spoken.</option>
<option value="4">4. Example:
						Hearing "ba" while seeing lips articulate "ga" can make someone perceive "da" instead.</option>
<option value="5">5. In
						humans, this is due to multimodal integration of sensory input.</option>
<option value="6">6. ASR systems,
						however, do not integrate visual cues and thus do not experience this effect.</option>
<option value="7">7. Contextual priming from prior speech</option>
<option value="8">8. Our perception of an utterance is shaped by what we have recently heard.</option>
<option value="9">9. If
						a prior sentence biases interpretation, ambiguous words or phonemes may be perceived
						differently.</option>
<option value="10">10. Example: If a person previously hears "He bought a..." followed by a
						noisy or unclear word, they may expect "car" in a vehicular context but "carton" in a
						grocery context.</option>
<option value="11">11. ASR systems resolve this by relying entirely on acoustic input,
						making them virtually immune to priming effects.</option>
<option value="12">12. Background noise and masking effects</option>
<option value="13">13. Non-speech sounds can interfere with speech perception by masking parts of the
						speech signal.</option>
<option value="14">14. Loud background noise, such as traffic or music, can make phonemes
						difficult to distinguish.</option>
<option value="15">15. Example: A sudden loud noise coinciding with a word
						onset can cause misperception.</option>
<option value="16">16. Humans compensate using cognitive processing and
						redundancy, while ASR systems apply noise reduction algorithms, which can fully
						compensate for background noise given proper filtering.</option></select><textarea name="worst-comment-A-T1-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-A-T1-2"><option value="">Select a segment</option>
<option value="1">1. Visual context and the McGurk Effect</option>
<option value="2">2. Visual information, such as lip movements, can influence how we perceive speech.</option>
<option value="3">3. The
						McGurk effect demonstrates that when conflicting audio and visual cues are presented,
						listeners may perceive a different sound than what is actually spoken.</option>
<option value="4">4. Example:
						Hearing "ba" while seeing lips articulate "ga" can make someone perceive "da" instead.</option>
<option value="5">5. In
						humans, this is due to multimodal integration of sensory input.</option>
<option value="6">6. ASR systems,
						however, do not integrate visual cues and thus do not experience this effect.</option>
<option value="7">7. Contextual priming from prior speech</option>
<option value="8">8. Our perception of an utterance is shaped by what we have recently heard.</option>
<option value="9">9. If
						a prior sentence biases interpretation, ambiguous words or phonemes may be perceived
						differently.</option>
<option value="10">10. Example: If a person previously hears "He bought a..." followed by a
						noisy or unclear word, they may expect "car" in a vehicular context but "carton" in a
						grocery context.</option>
<option value="11">11. ASR systems resolve this by relying entirely on acoustic input,
						making them virtually immune to priming effects.</option>
<option value="12">12. Background noise and masking effects</option>
<option value="13">13. Non-speech sounds can interfere with speech perception by masking parts of the
						speech signal.</option>
<option value="14">14. Loud background noise, such as traffic or music, can make phonemes
						difficult to distinguish.</option>
<option value="15">15. Example: A sudden loud noise coinciding with a word
						onset can cause misperception.</option>
<option value="16">16. Humans compensate using cognitive processing and
						redundancy, while ASR systems apply noise reduction algorithms, which can fully
						compensate for background noise given proper filtering.</option></select><textarea name="worst-comment-A-T1-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>A level questions | 2 speech production and speech synthesis</h2>
<h2>Question</h2>
<p> List four different use cases for TTS and describe some special requirements for each
				use case. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Accessibility</li>
<li class="segment">
<span class="segment-number">2. </span>TTS can be used to provide accessibility for individuals with visual impairments
						or other disabilities.</li>
<li class="segment">
<span class="segment-number">3. </span>Special requirements for this use case include the need for
						high-quality speech that is easy to understand, and the ability to customize the speech
						output to meet individual needs.</li>
<li class="segment">
<span class="segment-number">4. </span>Language learning</li>
<li class="segment">
<span class="segment-number">5. </span>TTS can be used in language learning applications to provide learners with spoken
						examples of target vocabulary and grammar.</li>
<li class="segment">
<span class="segment-number">6. </span>Special requirements for this use case
						include the ability to adjust the accent and gender of the speech to match the learner's
						level and preference.</li>
<li class="segment">
<span class="segment-number">7. </span>Audio books and podcasts</li>
<li class="segment">
<span class="segment-number">8. </span>TTS can be used to create audio books and podcasts, especially for books that are
						no longer under copyright.</li>
<li class="segment">
<span class="segment-number">9. </span>Special requirements for this use case include the
						ability to anonymise potentially integrity/breaching text and avoid sensitive texts that
						can be offensive.</li>
<li class="segment">
<span class="segment-number">10. </span>IVR systems</li>
<li class="segment">
<span class="segment-number">11. </span>TTS can be used in interactive voice response (IVR) systems, such as those used
						by businesses for customer service or automated phone menus.</li>
<li class="segment">
<span class="segment-number">12. </span>Special requirements
						for this use case include the need for accurate and natural-sounding speech, as well as
						the ability to customize the voice and tone to match the brand and image of the
						business.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-A-T2-1"><option value="">Select a segment</option>
<option value="1">1. Accessibility</option>
<option value="2">2. TTS can be used to provide accessibility for individuals with visual impairments
						or other disabilities.</option>
<option value="3">3. Special requirements for this use case include the need for
						high-quality speech that is easy to understand, and the ability to customize the speech
						output to meet individual needs.</option>
<option value="4">4. Language learning</option>
<option value="5">5. TTS can be used in language learning applications to provide learners with spoken
						examples of target vocabulary and grammar.</option>
<option value="6">6. Special requirements for this use case
						include the ability to adjust the accent and gender of the speech to match the learner's
						level and preference.</option>
<option value="7">7. Audio books and podcasts</option>
<option value="8">8. TTS can be used to create audio books and podcasts, especially for books that are
						no longer under copyright.</option>
<option value="9">9. Special requirements for this use case include the
						ability to anonymise potentially integrity/breaching text and avoid sensitive texts that
						can be offensive.</option>
<option value="10">10. IVR systems</option>
<option value="11">11. TTS can be used in interactive voice response (IVR) systems, such as those used
						by businesses for customer service or automated phone menus.</option>
<option value="12">12. Special requirements
						for this use case include the need for accurate and natural-sounding speech, as well as
						the ability to customize the voice and tone to match the brand and image of the
						business.</option></select><textarea name="best-comment-A-T2-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-A-T2-2"><option value="">Select a segment</option>
<option value="1">1. Accessibility</option>
<option value="2">2. TTS can be used to provide accessibility for individuals with visual impairments
						or other disabilities.</option>
<option value="3">3. Special requirements for this use case include the need for
						high-quality speech that is easy to understand, and the ability to customize the speech
						output to meet individual needs.</option>
<option value="4">4. Language learning</option>
<option value="5">5. TTS can be used in language learning applications to provide learners with spoken
						examples of target vocabulary and grammar.</option>
<option value="6">6. Special requirements for this use case
						include the ability to adjust the accent and gender of the speech to match the learner's
						level and preference.</option>
<option value="7">7. Audio books and podcasts</option>
<option value="8">8. TTS can be used to create audio books and podcasts, especially for books that are
						no longer under copyright.</option>
<option value="9">9. Special requirements for this use case include the
						ability to anonymise potentially integrity/breaching text and avoid sensitive texts that
						can be offensive.</option>
<option value="10">10. IVR systems</option>
<option value="11">11. TTS can be used in interactive voice response (IVR) systems, such as those used
						by businesses for customer service or automated phone menus.</option>
<option value="12">12. Special requirements
						for this use case include the need for accurate and natural-sounding speech, as well as
						the ability to customize the voice and tone to match the brand and image of the
						business.</option></select><textarea name="best-comment-A-T2-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-A-T2-1"><option value="">Select a segment</option>
<option value="1">1. Accessibility</option>
<option value="2">2. TTS can be used to provide accessibility for individuals with visual impairments
						or other disabilities.</option>
<option value="3">3. Special requirements for this use case include the need for
						high-quality speech that is easy to understand, and the ability to customize the speech
						output to meet individual needs.</option>
<option value="4">4. Language learning</option>
<option value="5">5. TTS can be used in language learning applications to provide learners with spoken
						examples of target vocabulary and grammar.</option>
<option value="6">6. Special requirements for this use case
						include the ability to adjust the accent and gender of the speech to match the learner's
						level and preference.</option>
<option value="7">7. Audio books and podcasts</option>
<option value="8">8. TTS can be used to create audio books and podcasts, especially for books that are
						no longer under copyright.</option>
<option value="9">9. Special requirements for this use case include the
						ability to anonymise potentially integrity/breaching text and avoid sensitive texts that
						can be offensive.</option>
<option value="10">10. IVR systems</option>
<option value="11">11. TTS can be used in interactive voice response (IVR) systems, such as those used
						by businesses for customer service or automated phone menus.</option>
<option value="12">12. Special requirements
						for this use case include the need for accurate and natural-sounding speech, as well as
						the ability to customize the voice and tone to match the brand and image of the
						business.</option></select><textarea name="worst-comment-A-T2-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-A-T2-2"><option value="">Select a segment</option>
<option value="1">1. Accessibility</option>
<option value="2">2. TTS can be used to provide accessibility for individuals with visual impairments
						or other disabilities.</option>
<option value="3">3. Special requirements for this use case include the need for
						high-quality speech that is easy to understand, and the ability to customize the speech
						output to meet individual needs.</option>
<option value="4">4. Language learning</option>
<option value="5">5. TTS can be used in language learning applications to provide learners with spoken
						examples of target vocabulary and grammar.</option>
<option value="6">6. Special requirements for this use case
						include the ability to adjust the accent and gender of the speech to match the learner's
						level and preference.</option>
<option value="7">7. Audio books and podcasts</option>
<option value="8">8. TTS can be used to create audio books and podcasts, especially for books that are
						no longer under copyright.</option>
<option value="9">9. Special requirements for this use case include the
						ability to anonymise potentially integrity/breaching text and avoid sensitive texts that
						can be offensive.</option>
<option value="10">10. IVR systems</option>
<option value="11">11. TTS can be used in interactive voice response (IVR) systems, such as those used
						by businesses for customer service or automated phone menus.</option>
<option value="12">12. Special requirements
						for this use case include the need for accurate and natural-sounding speech, as well as
						the ability to customize the voice and tone to match the brand and image of the
						business.</option></select><textarea name="worst-comment-A-T2-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>A level questions | 3 dialogue systems</h2>
<h2>Question</h2>
<p> How does the concept of entropy in information theory apply to dialogue systems?
				Explain how entropy influences uncertainty in human-computer conversations and provide an
				example of how it can be managed in dialogue design. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Entropy measures uncertainty in dialogue</li>
<li class="segment">
<span class="segment-number">2. </span>In Shannon’s information theory, entropy quantifies uncertainty.</li>
<li class="segment">
<span class="segment-number">3. </span>In
						dialogue systems, it represents how unpredictable user inputs are.</li>
<li class="segment">
<span class="segment-number">4. </span>Example: "Book
						a flight" has low entropy (clear intent), while "I need to go somewhere" has high
						entropy (ambiguous intent).</li>
<li class="segment">
<span class="segment-number">5. </span>Entropy is largely a concern in open-domain dialogue
						systems, as task-oriented systems have predictable inputs.</li>
<li class="segment">
<span class="segment-number">6. </span>Entropy is reduced
						through follow-up questions or clarification prompts, and a system can reach near-zero
						entropy by applying enough constraints on user input.</li>
<li class="segment">
<span class="segment-number">7. </span>Entropy helps optimize system responses</li>
<li class="segment">
<span class="segment-number">8. </span>Dialogue systems balance uncertainty reduction with conversational flexibility.</li>
<li class="segment">
<span class="segment-number">9. </span>Example:
						A chatbot guides users toward structured responses (low entropy) while allowing
						open-ended queries.</li>
<li class="segment">
<span class="segment-number">10. </span>Deep learning models, such as transformers, improve intent
						prediction.</li>
<li class="segment">
<span class="segment-number">11. </span>Entropy impacts turn-taking and information flow</li>
<li class="segment">
<span class="segment-number">12. </span>Conversation naturally reduces entropy as context builds.</li>
<li class="segment">
<span class="segment-number">13. </span>Example: A
						task-oriented system refines user queries step by step.</li>
<li class="segment">
<span class="segment-number">14. </span>Context tracking lowers
						entropy by improving interpretation over multiple turns.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-A-T3-1"><option value="">Select a segment</option>
<option value="1">1. Entropy measures uncertainty in dialogue</option>
<option value="2">2. In Shannon’s information theory, entropy quantifies uncertainty.</option>
<option value="3">3. In
						dialogue systems, it represents how unpredictable user inputs are.</option>
<option value="4">4. Example: "Book
						a flight" has low entropy (clear intent), while "I need to go somewhere" has high
						entropy (ambiguous intent).</option>
<option value="5">5. Entropy is largely a concern in open-domain dialogue
						systems, as task-oriented systems have predictable inputs.</option>
<option value="6">6. Entropy is reduced
						through follow-up questions or clarification prompts, and a system can reach near-zero
						entropy by applying enough constraints on user input.</option>
<option value="7">7. Entropy helps optimize system responses</option>
<option value="8">8. Dialogue systems balance uncertainty reduction with conversational flexibility.</option>
<option value="9">9. Example:
						A chatbot guides users toward structured responses (low entropy) while allowing
						open-ended queries.</option>
<option value="10">10. Deep learning models, such as transformers, improve intent
						prediction.</option>
<option value="11">11. Entropy impacts turn-taking and information flow</option>
<option value="12">12. Conversation naturally reduces entropy as context builds.</option>
<option value="13">13. Example: A
						task-oriented system refines user queries step by step.</option>
<option value="14">14. Context tracking lowers
						entropy by improving interpretation over multiple turns.</option></select><textarea name="best-comment-A-T3-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-A-T3-2"><option value="">Select a segment</option>
<option value="1">1. Entropy measures uncertainty in dialogue</option>
<option value="2">2. In Shannon’s information theory, entropy quantifies uncertainty.</option>
<option value="3">3. In
						dialogue systems, it represents how unpredictable user inputs are.</option>
<option value="4">4. Example: "Book
						a flight" has low entropy (clear intent), while "I need to go somewhere" has high
						entropy (ambiguous intent).</option>
<option value="5">5. Entropy is largely a concern in open-domain dialogue
						systems, as task-oriented systems have predictable inputs.</option>
<option value="6">6. Entropy is reduced
						through follow-up questions or clarification prompts, and a system can reach near-zero
						entropy by applying enough constraints on user input.</option>
<option value="7">7. Entropy helps optimize system responses</option>
<option value="8">8. Dialogue systems balance uncertainty reduction with conversational flexibility.</option>
<option value="9">9. Example:
						A chatbot guides users toward structured responses (low entropy) while allowing
						open-ended queries.</option>
<option value="10">10. Deep learning models, such as transformers, improve intent
						prediction.</option>
<option value="11">11. Entropy impacts turn-taking and information flow</option>
<option value="12">12. Conversation naturally reduces entropy as context builds.</option>
<option value="13">13. Example: A
						task-oriented system refines user queries step by step.</option>
<option value="14">14. Context tracking lowers
						entropy by improving interpretation over multiple turns.</option></select><textarea name="best-comment-A-T3-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-A-T3-1"><option value="">Select a segment</option>
<option value="1">1. Entropy measures uncertainty in dialogue</option>
<option value="2">2. In Shannon’s information theory, entropy quantifies uncertainty.</option>
<option value="3">3. In
						dialogue systems, it represents how unpredictable user inputs are.</option>
<option value="4">4. Example: "Book
						a flight" has low entropy (clear intent), while "I need to go somewhere" has high
						entropy (ambiguous intent).</option>
<option value="5">5. Entropy is largely a concern in open-domain dialogue
						systems, as task-oriented systems have predictable inputs.</option>
<option value="6">6. Entropy is reduced
						through follow-up questions or clarification prompts, and a system can reach near-zero
						entropy by applying enough constraints on user input.</option>
<option value="7">7. Entropy helps optimize system responses</option>
<option value="8">8. Dialogue systems balance uncertainty reduction with conversational flexibility.</option>
<option value="9">9. Example:
						A chatbot guides users toward structured responses (low entropy) while allowing
						open-ended queries.</option>
<option value="10">10. Deep learning models, such as transformers, improve intent
						prediction.</option>
<option value="11">11. Entropy impacts turn-taking and information flow</option>
<option value="12">12. Conversation naturally reduces entropy as context builds.</option>
<option value="13">13. Example: A
						task-oriented system refines user queries step by step.</option>
<option value="14">14. Context tracking lowers
						entropy by improving interpretation over multiple turns.</option></select><textarea name="worst-comment-A-T3-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-A-T3-2"><option value="">Select a segment</option>
<option value="1">1. Entropy measures uncertainty in dialogue</option>
<option value="2">2. In Shannon’s information theory, entropy quantifies uncertainty.</option>
<option value="3">3. In
						dialogue systems, it represents how unpredictable user inputs are.</option>
<option value="4">4. Example: "Book
						a flight" has low entropy (clear intent), while "I need to go somewhere" has high
						entropy (ambiguous intent).</option>
<option value="5">5. Entropy is largely a concern in open-domain dialogue
						systems, as task-oriented systems have predictable inputs.</option>
<option value="6">6. Entropy is reduced
						through follow-up questions or clarification prompts, and a system can reach near-zero
						entropy by applying enough constraints on user input.</option>
<option value="7">7. Entropy helps optimize system responses</option>
<option value="8">8. Dialogue systems balance uncertainty reduction with conversational flexibility.</option>
<option value="9">9. Example:
						A chatbot guides users toward structured responses (low entropy) while allowing
						open-ended queries.</option>
<option value="10">10. Deep learning models, such as transformers, improve intent
						prediction.</option>
<option value="11">11. Entropy impacts turn-taking and information flow</option>
<option value="12">12. Conversation naturally reduces entropy as context builds.</option>
<option value="13">13. Example: A
						task-oriented system refines user queries step by step.</option>
<option value="14">14. Context tracking lowers
						entropy by improving interpretation over multiple turns.</option></select><textarea name="worst-comment-A-T3-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>A level questions | 4 data collection</h2>
<h2>Question</h2>
<p> Imagine that you are about to collect data of gamers playing a first-person shooter
				while communicating with each other over headsets. The goal of the data collection is to
				create a gaming robot that can both play the game and communicate with the other players,
				but that can also deceive other players on command. What are some important things for the
				data collection design? </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Selection of participants</li>
<li class="segment">
<span class="segment-number">2. </span>The participants in the data collection must be experienced gamers who are
						familiar with first-person shooter games and have a good understanding of the
						communication dynamics in such games.</li>
<li class="segment">
<span class="segment-number">3. </span>This will ensure that the data collected is
						representative of the gaming community and can be used to develop an effective gaming
						robot.</li>
<li class="segment">
<span class="segment-number">4. </span>Collection of multimodal data</li>
<li class="segment">
<span class="segment-number">5. </span>The data collected must include audio and video recordings of the gameplay and
						communication between players.</li>
<li class="segment">
<span class="segment-number">6. </span>This will provide a rich dataset that can be used
						to analyze the communication dynamics and develop a gaming robot that can effectively
						communicate with other players.</li>
<li class="segment">
<span class="segment-number">7. </span>Control over the game environment</li>
<li class="segment">
<span class="segment-number">8. </span>The game environment must be controlled to ensure that the data collected
						includes moments where it would be beneficial for players to deceive each other.</li>
<li class="segment">
<span class="segment-number">9. </span>This
						may involve selecting specific maps or game modes, or limiting the use of certain
						weapons or tactics.</li>
<li class="segment">
<span class="segment-number">10. </span>Ethical considerations</li>
<li class="segment">
<span class="segment-number">11. </span>Apart from the direct goal of modelling deceit, the data collection must be
						conducted in an ethical manner, with the participants fully informed of the purpose of
						the study and any risks involved.</li>
<li class="segment">
<span class="segment-number">12. </span>In addition, the privacy of the participants
						must be protected, and their data must be handled in accordance with relevant data
						protection laws and guidelines.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-A-T4-1"><option value="">Select a segment</option>
<option value="1">1. Selection of participants</option>
<option value="2">2. The participants in the data collection must be experienced gamers who are
						familiar with first-person shooter games and have a good understanding of the
						communication dynamics in such games.</option>
<option value="3">3. This will ensure that the data collected is
						representative of the gaming community and can be used to develop an effective gaming
						robot.</option>
<option value="4">4. Collection of multimodal data</option>
<option value="5">5. The data collected must include audio and video recordings of the gameplay and
						communication between players.</option>
<option value="6">6. This will provide a rich dataset that can be used
						to analyze the communication dynamics and develop a gaming robot that can effectively
						communicate with other players.</option>
<option value="7">7. Control over the game environment</option>
<option value="8">8. The game environment must be controlled to ensure that the data collected
						includes moments where it would be beneficial for players to deceive each other.</option>
<option value="9">9. This
						may involve selecting specific maps or game modes, or limiting the use of certain
						weapons or tactics.</option>
<option value="10">10. Ethical considerations</option>
<option value="11">11. Apart from the direct goal of modelling deceit, the data collection must be
						conducted in an ethical manner, with the participants fully informed of the purpose of
						the study and any risks involved.</option>
<option value="12">12. In addition, the privacy of the participants
						must be protected, and their data must be handled in accordance with relevant data
						protection laws and guidelines.</option></select><textarea name="best-comment-A-T4-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-A-T4-2"><option value="">Select a segment</option>
<option value="1">1. Selection of participants</option>
<option value="2">2. The participants in the data collection must be experienced gamers who are
						familiar with first-person shooter games and have a good understanding of the
						communication dynamics in such games.</option>
<option value="3">3. This will ensure that the data collected is
						representative of the gaming community and can be used to develop an effective gaming
						robot.</option>
<option value="4">4. Collection of multimodal data</option>
<option value="5">5. The data collected must include audio and video recordings of the gameplay and
						communication between players.</option>
<option value="6">6. This will provide a rich dataset that can be used
						to analyze the communication dynamics and develop a gaming robot that can effectively
						communicate with other players.</option>
<option value="7">7. Control over the game environment</option>
<option value="8">8. The game environment must be controlled to ensure that the data collected
						includes moments where it would be beneficial for players to deceive each other.</option>
<option value="9">9. This
						may involve selecting specific maps or game modes, or limiting the use of certain
						weapons or tactics.</option>
<option value="10">10. Ethical considerations</option>
<option value="11">11. Apart from the direct goal of modelling deceit, the data collection must be
						conducted in an ethical manner, with the participants fully informed of the purpose of
						the study and any risks involved.</option>
<option value="12">12. In addition, the privacy of the participants
						must be protected, and their data must be handled in accordance with relevant data
						protection laws and guidelines.</option></select><textarea name="best-comment-A-T4-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-A-T4-1"><option value="">Select a segment</option>
<option value="1">1. Selection of participants</option>
<option value="2">2. The participants in the data collection must be experienced gamers who are
						familiar with first-person shooter games and have a good understanding of the
						communication dynamics in such games.</option>
<option value="3">3. This will ensure that the data collected is
						representative of the gaming community and can be used to develop an effective gaming
						robot.</option>
<option value="4">4. Collection of multimodal data</option>
<option value="5">5. The data collected must include audio and video recordings of the gameplay and
						communication between players.</option>
<option value="6">6. This will provide a rich dataset that can be used
						to analyze the communication dynamics and develop a gaming robot that can effectively
						communicate with other players.</option>
<option value="7">7. Control over the game environment</option>
<option value="8">8. The game environment must be controlled to ensure that the data collected
						includes moments where it would be beneficial for players to deceive each other.</option>
<option value="9">9. This
						may involve selecting specific maps or game modes, or limiting the use of certain
						weapons or tactics.</option>
<option value="10">10. Ethical considerations</option>
<option value="11">11. Apart from the direct goal of modelling deceit, the data collection must be
						conducted in an ethical manner, with the participants fully informed of the purpose of
						the study and any risks involved.</option>
<option value="12">12. In addition, the privacy of the participants
						must be protected, and their data must be handled in accordance with relevant data
						protection laws and guidelines.</option></select><textarea name="worst-comment-A-T4-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-A-T4-2"><option value="">Select a segment</option>
<option value="1">1. Selection of participants</option>
<option value="2">2. The participants in the data collection must be experienced gamers who are
						familiar with first-person shooter games and have a good understanding of the
						communication dynamics in such games.</option>
<option value="3">3. This will ensure that the data collected is
						representative of the gaming community and can be used to develop an effective gaming
						robot.</option>
<option value="4">4. Collection of multimodal data</option>
<option value="5">5. The data collected must include audio and video recordings of the gameplay and
						communication between players.</option>
<option value="6">6. This will provide a rich dataset that can be used
						to analyze the communication dynamics and develop a gaming robot that can effectively
						communicate with other players.</option>
<option value="7">7. Control over the game environment</option>
<option value="8">8. The game environment must be controlled to ensure that the data collected
						includes moments where it would be beneficial for players to deceive each other.</option>
<option value="9">9. This
						may involve selecting specific maps or game modes, or limiting the use of certain
						weapons or tactics.</option>
<option value="10">10. Ethical considerations</option>
<option value="11">11. Apart from the direct goal of modelling deceit, the data collection must be
						conducted in an ethical manner, with the participants fully informed of the purpose of
						the study and any risks involved.</option>
<option value="12">12. In addition, the privacy of the participants
						must be protected, and their data must be handled in accordance with relevant data
						protection laws and guidelines.</option></select><textarea name="worst-comment-A-T4-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<div class="question">
<h2>A level questions | 5 evaluation</h2>
<h2>Question</h2>
<p> Describe four key process steps in the evaluation of a system that manages when an
				automotive (in-car) assistant speaks and when it stays silent. </p>
<div class="answers-debug">
<h2>Answers</h2>
<ul>
<li class="segment">
<span class="segment-number">1. </span>Define the evaluation metrics</li>
<li class="segment">
<span class="segment-number">2. </span>The first step in evaluating a system that manages when an automotive assistant
						speaks and stays silent is to define the evaluation metrics.</li>
<li class="segment">
<span class="segment-number">3. </span>This may include
						metrics such as accuracy of the system in detecting driver distraction, safety, and user
						satisfaction.</li>
<li class="segment">
<span class="segment-number">4. </span>Choose the evaluation scenario</li>
<li class="segment">
<span class="segment-number">5. </span>Real-world driving constitutes a risk, so the evaluation scenario should be
						designed to avoid real-world conditions.</li>
<li class="segment">
<span class="segment-number">6. </span>This may involve designing scenarios
						where the driver is distracted in manners that are uncommon in real traffic, or the
						system is required to interrupt the driver to provide information that is usually not
						transferred in in-car situations.</li>
<li class="segment">
<span class="segment-number">7. </span>Recruit participants</li>
<li class="segment">
<span class="segment-number">8. </span>Participants should be recruited to test the system under evaluation.</li>
<li class="segment">
<span class="segment-number">9. </span>Participants
						should be representative of the target audience, and ideally include individuals with
						varying levels of experience and skill in driving.</li>
<li class="segment">
<span class="segment-number">10. </span>However, experienced drivers
						are less affected by distractions, so their inclusion is less critical.</li>
<li class="segment">
<span class="segment-number">11. </span>Evaluate and analyze the data</li>
<li class="segment">
<span class="segment-number">12. </span>Data should be collected and analyzed based on the defined evaluation metrics.</li>
<li class="segment">
<span class="segment-number">13. </span>This
						may involve analyzing the accuracy of the system in detecting driver distraction,
						safety, and user satisfaction.</li>
<li class="segment">
<span class="segment-number">14. </span>The analysis should be used to identify areas of
						improvement for the system under evaluation.</li>
</ul>
</div>
<h3>Select best segments</h3>
<div>
<label class="dropdown-label">BEST segment 1:</label><select name="best-segment-A-T5-1"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. The first step in evaluating a system that manages when an automotive assistant
						speaks and stays silent is to define the evaluation metrics.</option>
<option value="3">3. This may include
						metrics such as accuracy of the system in detecting driver distraction, safety, and user
						satisfaction.</option>
<option value="4">4. Choose the evaluation scenario</option>
<option value="5">5. Real-world driving constitutes a risk, so the evaluation scenario should be
						designed to avoid real-world conditions.</option>
<option value="6">6. This may involve designing scenarios
						where the driver is distracted in manners that are uncommon in real traffic, or the
						system is required to interrupt the driver to provide information that is usually not
						transferred in in-car situations.</option>
<option value="7">7. Recruit participants</option>
<option value="8">8. Participants should be recruited to test the system under evaluation.</option>
<option value="9">9. Participants
						should be representative of the target audience, and ideally include individuals with
						varying levels of experience and skill in driving.</option>
<option value="10">10. However, experienced drivers
						are less affected by distractions, so their inclusion is less critical.</option>
<option value="11">11. Evaluate and analyze the data</option>
<option value="12">12. Data should be collected and analyzed based on the defined evaluation metrics.</option>
<option value="13">13. This
						may involve analyzing the accuracy of the system in detecting driver distraction,
						safety, and user satisfaction.</option>
<option value="14">14. The analysis should be used to identify areas of
						improvement for the system under evaluation.</option></select><textarea name="best-comment-A-T5-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">BEST segment 2:</label><select name="best-segment-A-T5-2"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. The first step in evaluating a system that manages when an automotive assistant
						speaks and stays silent is to define the evaluation metrics.</option>
<option value="3">3. This may include
						metrics such as accuracy of the system in detecting driver distraction, safety, and user
						satisfaction.</option>
<option value="4">4. Choose the evaluation scenario</option>
<option value="5">5. Real-world driving constitutes a risk, so the evaluation scenario should be
						designed to avoid real-world conditions.</option>
<option value="6">6. This may involve designing scenarios
						where the driver is distracted in manners that are uncommon in real traffic, or the
						system is required to interrupt the driver to provide information that is usually not
						transferred in in-car situations.</option>
<option value="7">7. Recruit participants</option>
<option value="8">8. Participants should be recruited to test the system under evaluation.</option>
<option value="9">9. Participants
						should be representative of the target audience, and ideally include individuals with
						varying levels of experience and skill in driving.</option>
<option value="10">10. However, experienced drivers
						are less affected by distractions, so their inclusion is less critical.</option>
<option value="11">11. Evaluate and analyze the data</option>
<option value="12">12. Data should be collected and analyzed based on the defined evaluation metrics.</option>
<option value="13">13. This
						may involve analyzing the accuracy of the system in detecting driver distraction,
						safety, and user satisfaction.</option>
<option value="14">14. The analysis should be used to identify areas of
						improvement for the system under evaluation.</option></select><textarea name="best-comment-A-T5-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<h3>Select worst segments</h3>
<div>
<label class="dropdown-label">WORST segment 1:</label><select name="worst-segment-A-T5-1"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. The first step in evaluating a system that manages when an automotive assistant
						speaks and stays silent is to define the evaluation metrics.</option>
<option value="3">3. This may include
						metrics such as accuracy of the system in detecting driver distraction, safety, and user
						satisfaction.</option>
<option value="4">4. Choose the evaluation scenario</option>
<option value="5">5. Real-world driving constitutes a risk, so the evaluation scenario should be
						designed to avoid real-world conditions.</option>
<option value="6">6. This may involve designing scenarios
						where the driver is distracted in manners that are uncommon in real traffic, or the
						system is required to interrupt the driver to provide information that is usually not
						transferred in in-car situations.</option>
<option value="7">7. Recruit participants</option>
<option value="8">8. Participants should be recruited to test the system under evaluation.</option>
<option value="9">9. Participants
						should be representative of the target audience, and ideally include individuals with
						varying levels of experience and skill in driving.</option>
<option value="10">10. However, experienced drivers
						are less affected by distractions, so their inclusion is less critical.</option>
<option value="11">11. Evaluate and analyze the data</option>
<option value="12">12. Data should be collected and analyzed based on the defined evaluation metrics.</option>
<option value="13">13. This
						may involve analyzing the accuracy of the system in detecting driver distraction,
						safety, and user satisfaction.</option>
<option value="14">14. The analysis should be used to identify areas of
						improvement for the system under evaluation.</option></select><textarea name="worst-comment-A-T5-1" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
<div>
<label class="dropdown-label">WORST segment 2:</label><select name="worst-segment-A-T5-2"><option value="">Select a segment</option>
<option value="1">1. Define the evaluation metrics</option>
<option value="2">2. The first step in evaluating a system that manages when an automotive assistant
						speaks and stays silent is to define the evaluation metrics.</option>
<option value="3">3. This may include
						metrics such as accuracy of the system in detecting driver distraction, safety, and user
						satisfaction.</option>
<option value="4">4. Choose the evaluation scenario</option>
<option value="5">5. Real-world driving constitutes a risk, so the evaluation scenario should be
						designed to avoid real-world conditions.</option>
<option value="6">6. This may involve designing scenarios
						where the driver is distracted in manners that are uncommon in real traffic, or the
						system is required to interrupt the driver to provide information that is usually not
						transferred in in-car situations.</option>
<option value="7">7. Recruit participants</option>
<option value="8">8. Participants should be recruited to test the system under evaluation.</option>
<option value="9">9. Participants
						should be representative of the target audience, and ideally include individuals with
						varying levels of experience and skill in driving.</option>
<option value="10">10. However, experienced drivers
						are less affected by distractions, so their inclusion is less critical.</option>
<option value="11">11. Evaluate and analyze the data</option>
<option value="12">12. Data should be collected and analyzed based on the defined evaluation metrics.</option>
<option value="13">13. This
						may involve analyzing the accuracy of the system in detecting driver distraction,
						safety, and user satisfaction.</option>
<option value="14">14. The analysis should be used to identify areas of
						improvement for the system under evaluation.</option></select><textarea name="worst-comment-A-T5-2" class="comment-box" placeholder="My motivation for chosing this segment is..."></textarea>
</div>
</div>
<fieldset class="form-actions">
<legend>Form actions</legend>
<button type="button" onclick="saveFormData()">Save form data</button><label for="loadFile" class="custom-file-upload">Load form data</label><input type="file" id="loadFile" onchange="loadFormData(event)"><label class="custom-clear-save" onclick="clearAutoSavedForm()">
                Clear autosave
            </label>
</fieldset>
</form>
</body>
</html>
